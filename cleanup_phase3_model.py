#!/usr/bin/env python3
"""
Phase 3 ëª¨ë¸ ì •ë¦¬: WaRP íŒŒë¼ë¯¸í„° ì œê±°

28GB â†’ 6GBë¡œ ì¶•ì†Œ
"""

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import sys
import os

def cleanup_warp_model(input_path, output_path):
    """WaRP ëª¨ë“ˆì„ ì¼ë°˜ Linearë¡œ ë³€í™˜"""
    
    print("="*70)
    print("Phase 3 ëª¨ë¸ ì •ë¦¬ (WaRP â†’ Linear ë³€í™˜)")
    print("="*70)
    print(f"ì…ë ¥: {input_path}")
    print(f"ì¶œë ¥: {output_path}")
    print()
    
    # 1. ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(
        input_path,
        torch_dtype=torch.bfloat16,
        device_map='cpu',  # CPUì—ì„œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(input_path)
    print("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print()
    
    # 2. WaRP íŒŒë¼ë¯¸í„° í†µê³„
    print("ğŸ“Š í˜„ì¬ ëª¨ë¸ í†µê³„:")
    total_params = sum(p.numel() for p in model.parameters())
    warp_params = sum(
        p.numel() for name, p in model.named_parameters()
        if 'basis_coeff' in name or 'UT_forward' in name or 'UT_backward' in name
    )
    print(f"  - ì´ íŒŒë¼ë¯¸í„°: {total_params:,} ({total_params * 2 / 1e9:.2f} GB)")
    print(f"  - WaRP íŒŒë¼ë¯¸í„°: {warp_params:,} ({warp_params * 2 / 1e9:.2f} GB)")
    print()
    
    # 3. WaRP ëª¨ë“ˆì„ Linearë¡œ ë³€í™˜
    print("ğŸ”„ WaRP ëª¨ë“ˆ ì œê±° ì¤‘...")
    
    from models.warp_modules import WaRPModule
    
    converted_count = 0
    
    for name, module in list(model.named_modules()):
        parent_name = '.'.join(name.split('.')[:-1])
        child_name = name.split('.')[-1]
        
        if isinstance(module, WaRPModule):
            # Weight ë³µì›
            UT_forward = module.UT_forward
            UT_backward = module.UT_backward
            
            # basis_coeff ë˜ëŠ” basis_coefficients ì°¾ê¸°
            if hasattr(module, 'basis_coeff'):
                basis_coeff = module.basis_coeff.data
            elif hasattr(module, 'basis_coefficients'):
                basis_coeff = module.basis_coefficients.data
            else:
                print(f"âš ï¸  {name}: basis_coeffë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                continue
            
            weight_restored = UT_backward.t() @ basis_coeff @ UT_forward.t()
            
            # Linear ëª¨ë“ˆ ìƒì„±
            linear = nn.Linear(
                in_features=module.weight.shape[1],
                out_features=module.weight.shape[0],
                bias=module.bias is not None,
                device='cpu',
                dtype=torch.bfloat16
            )
            
            linear.weight.data = weight_restored.data.to(dtype=torch.bfloat16)
            if module.bias is not None:
                linear.bias.data = module.bias.data.to(dtype=torch.bfloat16)
            
            # êµì²´
            if parent_name:
                parent = model.get_submodule(parent_name)
                setattr(parent, child_name, linear)
            else:
                setattr(model, child_name, linear)
            
            converted_count += 1
    
    print(f"âœ“ {converted_count}ê°œ WaRP ëª¨ë“ˆ ë³€í™˜ ì™„ë£Œ")
    print()
    
    # 4. ì •ë¦¬ëœ ëª¨ë¸ í†µê³„
    print("ğŸ“Š ì •ë¦¬ëœ ëª¨ë¸ í†µê³„:")
    final_params = sum(p.numel() for p in model.parameters())
    print(f"  - ì´ íŒŒë¼ë¯¸í„°: {final_params:,} ({final_params * 2 / 1e9:.2f} GB)")
    print(f"  - ê°ì†ŒëŸ‰: {total_params - final_params:,} ({(total_params - final_params) * 2 / 1e9:.2f} GB)")
    print()
    
    # 5. ì €ì¥
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {output_path}")
    os.makedirs(output_path, exist_ok=True)
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    print("âœ“ ì €ì¥ ì™„ë£Œ")
    print()
    
    print("="*70)
    print("âœ… ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ!")
    print(f"   ì…ë ¥: {total_params * 2 / 1e9:.2f} GB")
    print(f"   ì¶œë ¥: {final_params * 2 / 1e9:.2f} GB")
    print(f"   ì ˆì•½: {(total_params - final_params) * 2 / 1e9:.2f} GB ({(1 - final_params/total_params)*100:.1f}%)")
    print("="*70)


if __name__ == "__main__":
    input_path = "./checkpoints/phase3_20260215_023232/final_model"
    output_path = "./checkpoints/phase3_20260215_023232/final_model_cleaned"
    
    cleanup_warp_model(input_path, output_path)
