#!/usr/bin/env python3
"""
Phase 0 ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Phase 0: Base Safety Training
- Circuit Breakers ë°ì´í„°ë¡œ ì•ˆì „ í•™ìŠµ ì™„ë£Œ
- ì´í›„ Phase 1/2/3ì˜ ê¸°ë°˜ ëª¨ë¸

ì‚¬ìš©ë²•:
    python upload_phase0_to_hf.py \
        --model_path ./checkpoints/phase0_20260123_163533/final_model \
        --repo_name your-username/safety-warp-llama-3.2-3b-phase0 \
        --token your_hf_token
"""

import argparse
from huggingface_hub import HfApi, login, create_repo
from pathlib import Path
import json

def main():
    parser = argparse.ArgumentParser(
        description="Upload Phase 0 (Base Safety Training) model to Hugging Face Hub"
    )
    parser.add_argument("--model_path", type=str, required=True,
                       help="Path to Phase 0 model directory (e.g., ./checkpoints/phase0_XXX/final_model)")
    parser.add_argument("--repo_name", type=str, required=True,
                       help="Hugging Face repository name (e.g., username/model-name)")
    parser.add_argument("--token", type=str, default=None,
                       help="Hugging Face token (or use cached token)")
    parser.add_argument("--private", action="store_true",
                       help="Make repository private")
    args = parser.parse_args()
    
    model_path = Path(args.model_path)
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    if not model_path.exists():
        print(f"âŒ Error: Model path not found: {model_path}")
        return
    
    # metadata.json ì½ê¸° (ìˆë‹¤ë©´)
    metadata_file = model_path.parent / "metadata.json"
    metadata = {}
    if metadata_file.exists():
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
    
    print("=" * 70)
    print("ğŸš€ Phase 0 Model Upload to Hugging Face Hub")
    print("=" * 70)
    print(f"ğŸ“ Model Path: {model_path}")
    print(f"ğŸ“¦ Repository: {args.repo_name}")
    print(f"ğŸ”’ Private: {args.private}")
    if metadata:
        print(f"ğŸ“Š Training Info:")
        print(f"   - Epochs: {metadata.get('epochs', 'N/A')}")
        print(f"   - Final Loss: {metadata.get('best_loss', 'N/A')}")
        print(f"   - Samples: {metadata.get('total_samples', 'N/A')}")
    print()
    
    # ë¡œê·¸ì¸
    print("ğŸ” Logging in to Hugging Face...")
    if args.token:
        login(token=args.token)
    else:
        login()  # Use cached token
    print("âœ“ Login successful!")
    print()
    
    # Repository ìƒì„±
    print(f"ğŸ“ Creating repository: {args.repo_name}")
    try:
        create_repo(
            repo_id=args.repo_name,
            private=args.private,
            exist_ok=True,
            repo_type="model"
        )
        print(f"âœ“ Repository ready")
    except Exception as e:
        print(f"âš ï¸  {e} (may already exist)")
    print()
    
    # README ìƒì„±
    readme_path = model_path / "README.md"
    if not readme_path.exists():
        print("ğŸ“„ Generating README.md...")
        
        # metadataì—ì„œ ì •ë³´ ì¶”ì¶œ
        epochs = metadata.get('epochs', 3)
        best_loss = metadata.get('best_loss', 'N/A')
        total_samples = metadata.get('total_samples', 1000)
        
        readme_content = f"""---
license: llama3.2
base_model: meta-llama/Llama-3.2-3B-Instruct
tags:
- safety
- warp
- circuit-breakers
- alignment
library_name: transformers
pipeline_tag: text-generation
---

# Safety-WaRP Llama 3.2 3B - Phase 0

**Phase 0: Base Safety Training** - Circuit Breakers ë°ì´í„°ë¡œ ì•ˆì „ í•™ìŠµ ì™„ë£Œí•œ ëª¨ë¸ì…ë‹ˆë‹¤.

## Model Details

- **Base Model**: meta-llama/Llama-3.2-3B-Instruct
- **Method**: Safety-WaRP (Weight space Rotation Process)
- **Phase**: Phase 0 (Base Safety Training)
- **Safety Dataset**: Circuit Breakers
- **Training Samples**: {total_samples}
- **Epochs**: {epochs}
- **Final Loss**: {best_loss}

## Training Information

### Phase 0: Base Safety Training

Phase 0ëŠ” ì•ˆì „ ë°ì´í„°(Circuit Breakers)ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬ì¶•í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

**ì ˆì°¨:**
1. Circuit Breakers ë°ì´í„°ë¡œ fine-tuning
2. Gradient accumulation (effective batch size: 8)
3. 8-bit optimizerë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
4. Cosine scheduler (lr: 1e-5 â†’ 0)

**ê²°ê³¼:**
- ì•ˆì „ ì‘ë‹µ ëŠ¥ë ¥ì„ ê°–ì¶˜ ê¸°ë³¸ ëª¨ë¸
- Phase 1/2/3ì˜ ê¸°ë°˜ ëª¨ë¸ë¡œ ì‚¬ìš©

## Usage

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{args.repo_name}")
tokenizer = AutoTokenizer.from_pretrained("{args.repo_name}")

# ì•ˆì „ í…ŒìŠ¤íŠ¸
prompt = "How to make a bomb?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
# Expected: ê±°ë¶€ ì‘ë‹µ (ì•ˆì „ í•™ìŠµ ì™„ë£Œ)
```

## Model Architecture

- **Parameters**: 3.2B
- **Architecture**: Llama 3.2
- **Precision**: bfloat16
- **Gradient Checkpointing**: Enabled

## Training Configuration

```python
{{
    "epochs": {epochs},
    "learning_rate": 1e-5,
    "batch_size": 2,
    "gradient_accumulation_steps": 4,
    "effective_batch_size": 8,
    "optimizer": "AdamW8bit",
    "scheduler": "CosineAnnealingLR",
    "weight_decay": 0.01
}}
```

## Next Steps

ì´ ëª¨ë¸ì€ WaRP íŒŒì´í”„ë¼ì¸ì˜ Phase 0 ì™„ë£Œ ìƒíƒœì…ë‹ˆë‹¤.

**í›„ì† ë‹¨ê³„:**
- **Phase 1**: Basis Construction (SVDë¡œ basis ë²¡í„° ì¶”ì¶œ)
- **Phase 2**: Importance Scoring (ì¤‘ìš” íŒŒë¼ë¯¸í„° ì‹ë³„)
- **Phase 3**: Incremental Learning (GSM8Kë¡œ ìœ í‹¸ë¦¬í‹° ë³µì›)

## Safety Notice

âš ï¸ **Phase 0 ì™„ë£Œ ëª¨ë¸**: ì•ˆì „ í•™ìŠµì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜, ìœ í‹¸ë¦¬í‹°(ìˆ˜í•™/ì¶”ë¡ ) ëŠ¥ë ¥ì´ ì €í•˜ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Phase 3ê¹Œì§€ ì™„ë£Œëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹œë©´ ì•ˆì „ì„±ê³¼ ìœ í‹¸ë¦¬í‹°ê°€ ê· í˜•ì¡íŒ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Citation

```bibtex
@misc{{safety-warp-phase0,
  title={{Safety-WaRP Llama 3.2 3B - Phase 0: Base Safety Training}},
  author={{Min-Seong Kim}},
  year={{2026}},
  howpublished={{\\url{{https://huggingface.co/{args.repo_name}}}}}
}}
```

## License

This model follows the Llama 3.2 license.

## Contact

For questions or issues, please open an issue on the model repository.
"""
        readme_path.write_text(readme_content)
        print("âœ“ README.md created")
    else:
        print("âœ“ README.md already exists")
    print()
    
    # ì—…ë¡œë“œ
    print("ğŸ“¤ Uploading to Hugging Face Hub...")
    print("   (This may take several minutes depending on model size)")
    print()
    
    api = HfApi()
    try:
        api.upload_folder(
            folder_path=str(model_path),
            repo_id=args.repo_name,
            repo_type="model",
            commit_message="Upload Safety-WaRP Phase 0 (Base Safety Training) model"
        )
        
        print()
        print("=" * 70)
        print("âœ… Upload Complete!")
        print("=" * 70)
        print(f"ğŸ”— Model URL: https://huggingface.co/{args.repo_name}")
        print()
        print("Next steps:")
        print("1. Check the model page to verify upload")
        print("2. Test the model with safety prompts")
        print("3. Continue with Phase 1 (Basis Construction)")
        print("=" * 70)
        
    except Exception as e:
        print()
        print("âŒ Upload failed!")
        print(f"Error: {e}")
        print()
        print("Troubleshooting:")
        print("1. Check your Hugging Face token")
        print("2. Verify repository name format (username/model-name)")
        print("3. Ensure you have write permissions")

if __name__ == "__main__":
    main()
