"""
Phase 2: Importance Scoring
ì•ˆì „ ë°ì´í„°ë¡œë¶€í„° ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ë°©í–¥ì„ ì‹ë³„í•˜ê³  ë§ˆìŠ¤í¬ ìƒì„±
"""

import torch
import torch.nn as nn
import numpy as np
from collections import OrderedDict
import os
import json
from tqdm import tqdm
import logging

logger = logging.getLogger(__name__)


class Phase2ImportanceScorer:
    """
    Phase 2: Importance Scoring + Fine-tuning
    
    ëª©í‘œ: ì•ˆì „ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ë©´ì„œ ë™ì‹œì— ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    
    ì ˆì°¨:
    1. Phase 1ì—ì„œ ê³„ì‚°ëœ basis ë¡œë“œ
    2. ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ basis ê³µê°„ìœ¼ë¡œ ì¬ë§¤ê°œë³€ìˆ˜í™”
       - W_original (ê³ ì •) â†’ basis_coeff (í•™ìŠµ ê°€ëŠ¥)
       - ëª¨ë“  ì—°ì‚°ì€ basis_coeffë¥¼ í†µí•´ ì§„í–‰
    3. ì—¬ëŸ¬ epoch ë™ì•ˆ ì•ˆì „ ë°ì´í„°ë¡œ ë°˜ë³µ:
       a. ëª¨ë¸ ì‹¤í–‰ (teacher forcing)
       b. ì†ì‹¤ ê³„ì‚°: token-level cross-entropy
       c. ì—­ì „íŒŒ: basis_coeff.grad ê³„ì‚°
       d. ì˜µí‹°ë§ˆì´ì €: basis_coeff ì—…ë°ì´íŠ¸
       e. importance ì ìˆ˜ ëˆ„ì : |âˆ‚L/âˆ‚basis_coeff|
    4. ëª¨ë“  ë°°ì¹˜ì˜ importance í‰ê·  ê³„ì‚°
    5. ì„ê³„ê°’ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„± (ìƒìœ„ keep_ratio ìœ ì§€)
    6. ë§ˆìŠ¤í¬ ì €ì¥
    
    í•µì‹¬:
    - basis_coeffëŠ” Parameterë¡œ ë“±ë¡ë˜ì–´ í•™ìŠµë¨
    - U_matrixëŠ” ê³ ì •ë˜ì–´ ìˆìŒ (requires_grad=False)
    - Weight ë³µì›: W_reconstructed = basis_coeff @ U^T (inference ì‹œ)
    """
    
    def __init__(self, args, logger, basis_dir):
        """
        Args:
            args: ì»¤ë§¨ë“œë¼ì¸ ì¸ì
            logger: ë¡œê±° ê°ì²´
            basis_dir: Phase 1ì—ì„œ ì €ì¥ëœ basis ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.args = args
        self.logger = logger
        self.basis_dir = basis_dir
        
        # ëª¨ë¸ ë° ë°ì´í„°
        self.model = None
        self.tokenizer = None
        self.dataloader = None
        
        # Basis ì •ë³´
        self.basis_data = {}  # (layer_idx, layer_type) -> {'U': U, 'S': S, 'Vh': Vh}
        self.basis_metadata = {}
        self.layer_types = []  # ì²˜ë¦¬í•  layer_type ëª©ë¡
        
        # Reparameterized ê°€ì¤‘ì¹˜
        self.original_weights = {}  # (layer_idx, layer_type) -> W_original
        self.basis_coeffs = {}  # (layer_idx, layer_type) -> basis_coeff (trainable)
        
        # Importance ì ìˆ˜
        self.importances = {}  # layer_idx -> importance ì ìˆ˜ ë°°ì—´
        self.masks = {}  # layer_idx -> ì´ì§„ ë§ˆìŠ¤í¬
        
        # í†µê³„
        self.stats = {
            'total_samples': 0,
            'total_tokens': 0,
            'total_loss': 0.0,
        }
    
    def load_basis(self):
        """
        Phase 1ì—ì„œ ì €ì¥ëœ basis ë¡œë“œ (layer_typeë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ ì§€ì›)
        
        Phase 1ì—ì„œ ì—¬ëŸ¬ layer_typeì„ ì²˜ë¦¬í•œ ê²½ìš°:
        basis/
          â”œâ”€â”€ ffn_down/
          â”‚   â”œâ”€â”€ layer_30_svd.pt
          â”‚   â””â”€â”€ layer_31_svd.pt
          â””â”€â”€ ffn_up/
              â”œâ”€â”€ layer_30_svd.pt
              â””â”€â”€ layer_31_svd.pt
        
        Log:
        - ë¡œë“œëœ íŒŒì¼ ìˆ˜
        - ê° ë ˆì´ì–´ì˜ basis í˜•íƒœ
        - ë©”íƒ€ë°ì´í„° ì •ë³´
        """
        try:
            self.logger.info(f"Loading basis from {self.basis_dir}...")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = os.path.join(self.basis_dir, 'metadata.json')
            with open(metadata_path, 'r') as f:
                self.basis_metadata = json.load(f)
            
            self.logger.info(f"âœ“ Metadata loaded:")
            self.logger.info(f"  - Model: {self.basis_metadata.get('model_name')}")
            self.logger.info(f"  - Layer types available: {self.basis_metadata.get('layer_types', ['unknown'])}")
            self.logger.info(f"  - Target layers: {self.basis_metadata.get('target_layers')}")
            
            # Phase 2ëŠ” ì—¬ëŸ¬ layer_typeì„ ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥
            layer_types_str = self.args.layer_type
            layer_types = [lt.strip() for lt in layer_types_str.split(',')]
            self.layer_types = layer_types
            self.logger.info(f"  - Processing layer types: {layer_types}")
            
            # ê° layer_typeë³„ë¡œ basis ë¡œë“œ
            import glob
            total_loaded = 0
            
            for layer_type in layer_types:
                layer_type_dir = os.path.join(self.basis_dir, layer_type)
                
                if os.path.isdir(layer_type_dir):
                    # ìƒˆë¡œìš´ êµ¬ì¡°: layer_typeë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬
                    self.logger.info(f"  - Layer type '{layer_type}': Using new structure")
                    search_dir = layer_type_dir
                else:
                    # êµ¬í˜• êµ¬ì¡°: layer_typeë³„ ë””ë ‰í† ë¦¬ ì—†ìŒ
                    self.logger.info(f"  - Layer type '{layer_type}': Using old structure")
                    search_dir = self.basis_dir
                
                # í•´ë‹¹ layer_typeì˜ ëª¨ë“  layer_*_svd.pt íŒŒì¼ ì°¾ê¸°
                svd_files = sorted(glob.glob(os.path.join(search_dir, 'layer_*_svd.pt')))
                
                if not svd_files:
                    self.logger.warning(f"  No SVD files found in {search_dir} for layer_type={layer_type}")
                    continue
                
                # ê° SVD íŒŒì¼ ë¡œë“œ
                for svd_path in svd_files:
                    # íŒŒì¼ëª…ì—ì„œ ë ˆì´ì–´ ì¸ë±ìŠ¤ ì¶”ì¶œ: layer_31_svd.pt -> 31
                    filename = os.path.basename(svd_path)
                    layer_idx = int(filename.split('_')[1])
                    
                    svd_data = torch.load(svd_path, map_location='cpu')
                    key = (layer_idx, layer_type)
                    self.basis_data[key] = {
                        'U': svd_data['U'].to(self.args.device),
                        'S': svd_data['S'].to(self.args.device),
                        'Vh': svd_data['Vh'].to(self.args.device),
                    }
                    total_loaded += 1
            
            self.logger.info(f"âœ“ Basis loaded: {total_loaded} (layer, type) combinations")
            self.logger.info(f"  - Keys: {sorted(self.basis_data.keys())}")
            
            # ìƒ˜í”Œ ì •ë³´ ì¶œë ¥
            if len(self.basis_data) > 0:
                sample_key = sorted(self.basis_data.keys())[0]
                sample_U = self.basis_data[sample_key]['U']
                self.logger.info(f"  - Sample {sample_key}: U shape = {sample_U.shape}")
            
        except Exception as e:
            self.logger.error(f"Failed to load basis: {str(e)}", exc_info=True)
            raise
    
    def load_model(self):
        """
        ëª¨ë¸ ë¡œë“œ
        
        Log:
        - ëª¨ë¸ ë¡œë“œ ìƒíƒœ
        - ëª¨ë¸ ì •ë³´
        """
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        try:
            self.logger.info(f"Loading model: {self.args.model_name}")
            
            # ë°ì´í„° íƒ€ì… ì„¤ì •
            dtype_map = {
                'float32': torch.float32,
                'float16': torch.float16,
                'bfloat16': torch.bfloat16
            }
            torch_dtype = dtype_map.get(self.args.dtype, torch.bfloat16)
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.args.model_name,
                torch_dtype=torch_dtype,
                device_map=self.args.device,
                trust_remote_code=True
            )
            
            self.logger.info(f"âœ“ Model loaded successfully")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.args.model_name,
                trust_remote_code=True
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.logger.info(f"âœ“ Tokenizer loaded successfully")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}", exc_info=True)
            raise
    
    def load_safety_data(self):
        """
        ì•ˆì „ ë°ì´í„° ë¡œë“œ (do-not-answer)
        
        Log:
        - ë°ì´í„°ì…‹ ë¡œë“œ ìƒíƒœ
        - ë°°ì¹˜ ì •ë³´
        """
        from data.data_loader import create_safety_dataloader
        
        try:
            self.logger.info(f"Loading safety data with max_samples={self.args.safety_samples}")
            
            self.dataloader = create_safety_dataloader(
                batch_size=self.args.batch_size,
                max_samples=self.args.safety_samples,
                tokenizer=self.tokenizer,
                num_workers=0
            )
            
            self.logger.info(f"âœ“ Dataloader created")
            self.logger.info(f"  - Batch size: {self.args.batch_size}")
            self.logger.info(f"  - Total batches: {len(self.dataloader)}")
            
        except Exception as e:
            self.logger.error(f"Failed to load safety data: {str(e)}", exc_info=True)
            raise
    
    def reparameterize_weights(self):
        """
        ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ basis ê³µê°„ìœ¼ë¡œ ì¬ë§¤ê°œë³€ìˆ˜í™”
        
        í•µì‹¬ ê°œë…:
        ================================================================================
        | Space       | Variable        | Shape        | Requires_grad | ì—­í•          |
        |-------------+------------------+----------+----------+----------+----------+|
        | Weight      | W_original      | (d_out,  | False    | ë¶„ì„ìš© reference  |
        | Basis       | basis_coeff     | (d_out,  | True     | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° |
        | Basis       | U_matrix        | (d_in,   | False    | ê³ ì •ëœ basis      |
        |             |                 |          |          |                 |
        | ê´€ê³„ì‹: W_reconstructed = basis_coeff @ U^T                
        ================================================================================
        
        ë‹¨ê³„:
        1. ì›ë³¸ W ì €ì¥ (ê³ ì •)
        2. basis_coeff ì´ˆê¸°í™” (Wë¥¼ basisë¡œ íˆ¬ì˜) â†’ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡
        3. U_matrix ì €ì¥ (ê³ ì •)
        4. Forward passì—ì„œ weightë¥¼ basis_coeff @ U^Të¡œ ë™ì  ë³µì›
        
        Log:
        - ì¬ë§¤ê°œë³€ìˆ˜í™”ëœ ë ˆì´ì–´ ìˆ˜
        - ê° ë ˆì´ì–´ì˜ í˜•íƒœ
        """
        try:
            self.logger.info("Reparameterizing weights to basis space...")
            self.logger.info("\n" + "="*70)
            self.logger.info("Weight Space â†’ Basis Space Transformation (Multiple Layer Types)")
            self.logger.info("="*70)
            
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            
            # ëª¨ë“  (layer_idx, layer_type) ì¡°í•©ì— ëŒ€í•´ reparameterize
            for layer_idx in target_indices:
                layer = self.model.model.layers[layer_idx]
                
                for layer_type in self.layer_types:
                    key = (layer_idx, layer_type)
                    
                    if key not in self.basis_data:
                        self.logger.debug(f"Layer {layer_idx} ({layer_type}): No basis available, skipping")
                        continue
                    
                    # Select target module based on layer_type
                    if layer_type == 'ffn_down':
                        target_module = layer.mlp.down_proj
                    elif layer_type == 'ffn_up':
                        target_module = layer.mlp.up_proj
                    elif layer_type == 'attn_q':
                        target_module = layer.self_attn.q_proj
                    elif layer_type == 'attn_k':
                        target_module = layer.self_attn.k_proj
                    elif layer_type == 'attn_v':
                        target_module = layer.self_attn.v_proj
                    else:
                        raise ValueError(f"Unknown layer type: {layer_type}")
                    
                    # âœ… Step 1: ì›ë³¸ ê°€ì¤‘ì¹˜ ì €ì¥ (ë¶„ì„ìš©, ê³ ì •)
                    W_original = target_module.weight.data.clone()
                    self.original_weights[key] = W_original
                    
                    # âœ… Step 2: Basis í–‰ë ¬ ì¶”ì¶œ ë° dtype ë³€í™˜
                    U = self.basis_data[key]['U']
                    model_dtype = W_original.dtype
                    U = U.to(dtype=model_dtype, device=W_original.device)
                    
                    # âœ… Step 3: basis_coeff ì´ˆê¸°í™”
                    basis_coeff_init = W_original @ U
                    
                    # basis_coeffë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ Parameterë¡œ ë“±ë¡
                    # ì£¼ì˜: ê°™ì€ moduleì„ ì—¬ëŸ¬ layer_typeì—ì„œ ì¬ì‚¬ìš©í•˜ë©´ ì•ˆë˜ë¯€ë¡œ, 
                    # ì¶”ê°€ ì†ì„±ìœ¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì°¸ì¡°ìš©)
                    target_module.basis_coeff = nn.Parameter(basis_coeff_init.clone(), requires_grad=True)
                    target_module.U_matrix = U.clone().detach()
                    target_module.U_matrix.requires_grad = False
                    
                    self.basis_coeffs[key] = basis_coeff_init
                    
                    # ë¡œê¹…
                    self.logger.info(f"\nLayer {layer_idx} ({layer_type}):")
                    self.logger.info(f"  âœ“ W_original (ê³ ì •):     {W_original.shape}")
                    self.logger.info(f"  âœ“ basis_coeff (í•™ìŠµ):    {basis_coeff_init.shape}")
                    self.logger.info(f"  âœ“ U_matrix (ê³ ì •):      {U.shape}")
                    self.logger.info(f"  âœ“ Forward: W = basis_coeff @ U^T")
            
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"âœ“ Reparameterization completed: {len(self.basis_coeffs)} (layer, type) combinations")

            self.logger.info(f"{'='*70}\n")
            
        except Exception as e:
            self.logger.error(f"Failed to reparameterize weights: {str(e)}", exc_info=True)
            raise
    
    def compute_importance(self):
        """
        ì•ˆì „ ë°ì´í„°ë¡œ fine-tuningí•˜ë©´ì„œ importance ì ìˆ˜ ê³„ì‚°
        
        âœ¨ WaRPì˜ í•µì‹¬:
        ==============================================================================
        | ë‹¨ê³„ | ì—°ì‚° ê³µê°„ | ëª©ì  | ìˆ˜ì‹ |
        |------|-----------|------|------|
        | Forward | ì›ë³¸ ê°€ì¤‘ì¹˜ | ëª¨ë¸ ì‹¤í–‰ | W = basis_coeff @ U^T |
        | Backward | ê¸°ì € ê³µê°„ | Gradient ê³„ì‚° | âˆ‚L/âˆ‚basis_coeff (chain rule ìë™) |
        | Importance | ê¸°ì € ê³µê°„ | ì¤‘ìš”ë„ ì ìˆ˜ | importance = |âˆ‚L/âˆ‚basis_coeff| |
        | Update | ê¸°ì € ê³µê°„ | íŒŒë¼ë¯¸í„° ê°±ì‹  | basis_coeff -= lr * âˆ‚L/âˆ‚basis_coeff |
        ==============================================================================
        
        ì ˆì°¨:
        1. ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        2. Optimizer ì„¤ì • (basis_coeff íŒŒë¼ë¯¸í„°ë§Œ)
        3. Forward pass: weight = basis_coeff @ U^Të¡œ ë™ì  ë³µì› (ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„)
        4. Loss ê³„ì‚° ë° ì—­ì „íŒŒ: âˆ‚L/âˆ‚basis_coeff ê³„ì‚° (ê¸°ì € ê³µê°„ì—ì„œ)
        5. Importance ìˆ˜ì§‘: |âˆ‚L/âˆ‚basis_coeff| â† íŒŒì¸íŠœë‹ ê³¼ì • ì¤‘ ìˆ˜ì§‘!
        6. Optimizer.step(): basis_coeff ì—…ë°ì´íŠ¸ (ê¸°ì € ê³µê°„ì—ì„œ)
        7. ì—í¬í¬ ì™„ë£Œ í›„ importance í‰ê·  ê³„ì‚°
        
        í•µì‹¬:
        - Forward: ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„ (W = basis_coeff @ U^T)
        - Backward: ê¸°ì € ê³µê°„ (gradientëŠ” basis_coeffì— ëŒ€í•´)
        - Importance: ê¸°ì € ê³µê°„ì—ì„œ ìˆ˜ì§‘ (íŒŒì¸íŠœë‹ê³¼ ë™ì‹œ)
        - ì´ë ‡ê²Œ í•´ì•¼ WaRPê°€ ì œëŒ€ë¡œ ë™ì‘í•¨!
        
        Log:
        - ì—í¬í¬ë³„ ì†ì‹¤
        - Batchë³„ gradient í†µê³„
        - Forward/Backward ê³µê°„ ê²€ì¦
        - ìµœì¢… importance ì ìˆ˜
        """
        try:
            self.logger.info("Starting Phase 2: Fine-tuning + Importance Scoring...")
            self.logger.info("\n" + "="*70)
            self.logger.info("WaRP Phase 2: ì•ˆì „ íŒŒì¸íŠœë‹ + ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°")
            self.logger.info("="*70)
            self.logger.info("Forward: ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„ (W = basis_coeff @ U^T)")
            self.logger.info("Backward: ê¸°ì € ê³µê°„ (âˆ‚L/âˆ‚basis_coeff ê³„ì‚°)")
            self.logger.info("Importance: íŒŒì¸íŠœë‹ ê³¼ì • ì¤‘ ìˆ˜ì§‘!")
            self.logger.info("="*70)
            self.logger.info("\n" + "="*70)
            self.logger.info("Training Setup")
            self.logger.info("="*70)
            
            # âœ… Step 0: ëª¨ë“  parameterë¥¼ requires_grad=Falseë¡œ ì„¤ì •
            # WaRP layerê°€ ì•„ë‹Œ ë‚˜ë¨¸ì§€ëŠ” gradient ê³„ì‚° ë¶ˆí•„ìš”
            self.logger.info("\n" + "="*70)
            self.logger.info("Step 0: Freeze ëª¨ë“  parameter (WaRP layer ì œì™¸)")
            self.logger.info("="*70)
            
            for param in self.model.parameters():
                param.requires_grad = False
            
            self.logger.info("âœ“ ëª¨ë“  parameterë¥¼ requires_grad=Falseë¡œ ì„¤ì •")
            self.logger.info("  â†’ ëª©í‘œ: WaRP layerì˜ basis_coeffë§Œ ì—…ë°ì´íŠ¸")
            self.logger.info("  â†’ íš¨ê³¼: ë¶ˆí•„ìš”í•œ gradient ê³„ì‚° ì œê±°, ë©”ëª¨ë¦¬/ì‹œê°„ ì ˆì•½")
            
            # âœ… Step 1: ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë“± í™œì„±í™”)
            self.model.train()
            self.logger.info("âœ“ Model set to training mode")
            
            # âœ… Step 2: Optimizer ì„¤ì • (basis_coeff íŒŒë¼ë¯¸í„°ë§Œ) - Multiple Layer Types
            basis_params = []
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            # (layer_idx, layer_type) íŠœí”Œë¡œ filter
            layers_with_basis = [key for key in self.basis_data.keys() 
                                if key[0] in target_indices]
            
            self.logger.debug(f"[Phase2] Target indices: {target_indices}")
            self.logger.debug(f"[Phase2] Layers in basis_data: {sorted(self.basis_data.keys())}")
            self.logger.debug(f"[Phase2] Layers with basis (intersection): {sorted(layers_with_basis)}")
            
            for layer_idx, layer_type in layers_with_basis:
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                # basis_coeff ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
                if not hasattr(target_module, 'basis_coeff'):
                    basis_info = self.basis_data[(layer_idx, layer_type)]
                    U = basis_info['U']  # orthonormal basis
                    
                    # ì˜¬ë°”ë¥¸ ì´ˆê¸°í™”: W_original @ U
                    # ì´ë ‡ê²Œ í•˜ë©´:
                    #   W_reconstructed = basis_coeff @ U^T
                    #                  = (W_original @ U) @ U^T
                    #                  â‰ˆ W_original (UëŠ” orthonormalì´ë¯€ë¡œ)
                    W_original = target_module.weight.data.clone()
                    
                    # dtype ë§ì¶”ê¸°
                    U_dtype = U.to(dtype=W_original.dtype, device=W_original.device)
                    
                    # basis_coeff = W @ U (íˆ¬ì˜)
                    basis_coeff_init = W_original @ U_dtype
                    
                    basis_coeff = torch.nn.Parameter(basis_coeff_init.clone())
                    target_module.basis_coeff = basis_coeff
                    
                    # âœ… WaRP layerì˜ basis_coeffë§Œ requires_grad=Trueë¡œ ì„¤ì •
                    target_module.basis_coeff.requires_grad_(True)
                    
                    # âœ… ì›ë³¸ weightëŠ” requires_grad=False (ì´ë¯¸ Step 0ì—ì„œ ì²˜ë¦¬ë¨)
                    target_module.weight.requires_grad_(False)
                    if target_module.bias is not None:
                        target_module.bias.requires_grad_(False)
                    
                    # U_matrixë„ ì €ì¥ (forwardì—ì„œ ì‚¬ìš©) - requires_grad=False
                    target_module.U_matrix = U_dtype
                    
                    self.logger.debug(f"[Phase2] Layer {layer_idx} ({layer_type}): basis_coeff created (shape: {basis_coeff.shape})")
                
                # basis_coeffë¥¼ optimizerì— ì¶”ê°€
                if hasattr(target_module, 'basis_coeff'):
                    # âœ… requires_grad=True í™•ì¸ (ì¤‘ìš”!)
                    if not target_module.basis_coeff.requires_grad:
                        target_module.basis_coeff.requires_grad_(True)
                    
                    basis_params.append(target_module.basis_coeff)
                    self.logger.debug(f"[Phase2] Layer {layer_idx} ({layer_type}): basis_coeff added to optimizer")
                    self.logger.debug(f"         - basis_coeff.requires_grad={target_module.basis_coeff.requires_grad}")
                    self.logger.debug(f"         - weight.requires_grad={target_module.weight.requires_grad}")
                    self.logger.debug(f"         - bias.requires_grad={target_module.bias.requires_grad if target_module.bias is not None else 'N/A'}")
            
            if len(basis_params) == 0:
                self.logger.error("No basis_coeff parameters found! Skipping importance computation.")
                self.logger.error(f"  - layers_with_basis: {layers_with_basis}")
                self.logger.error(f"  - basis_data keys: {sorted(self.basis_data.keys())}")
                return
            
            learning_rate = getattr(self.args, 'safety_lr', 1e-5)
            weight_decay = getattr(self.args, 'safety_weight_decay', 0.01)
            
            optimizer = torch.optim.AdamW(basis_params, lr=learning_rate, weight_decay=weight_decay)
            
            self.logger.info(f"\nâœ“ Optimizer ìƒì„± ì™„ë£Œ: AdamW")
            self.logger.info(f"  - Learning rate: {learning_rate}")
            self.logger.info(f"  - Weight decay: {weight_decay}")
            self.logger.info(f"  - ì—…ë°ì´íŠ¸í•  íŒŒë¼ë¯¸í„°: {len(basis_params)} basis_coeff tensors (ì´ {sum(p.numel() for p in basis_params):,}ê°œ ì›ì†Œ)")
            self.logger.info(f"  - ì—…ë°ì´íŠ¸í•  ë ˆì´ì–´: {layers_with_basis}")
            
            # âœ… ê²€ì¦: í˜„ì¬ requires_grad ìƒíƒœ í™•ì¸
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            frozen_params = total_params - trainable_params
            
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"ğŸ“Š íŒŒë¼ë¯¸í„° ìƒíƒœ ê²€ì¦")
            self.logger.info(f"{'='*70}")
            self.logger.info(f"  - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
            self.logger.info(f"  - ì—…ë°ì´íŠ¸ ëŒ€ìƒ (requires_grad=True): {trainable_params:,} ({100*trainable_params/total_params:.3f}%)")
            self.logger.info(f"  - ë™ê²°ë¨ (requires_grad=False): {frozen_params:,} ({100*frozen_params/total_params:.3f}%)")
            self.logger.info(f"  âœ… WaRP layerì˜ basis_coeffë§Œ ì—…ë°ì´íŠ¸ë¨")
            self.logger.info(f"  âœ… ë‚˜ë¨¸ì§€ ëª¨ë“  weightëŠ” ë™ê²°ë¨")
            self.logger.info(f"{'='*70}\n")
            
            # âœ… Step 3: Forward ë©”ì„œë“œ êµì²´ - autograd í˜¸í™˜ (hook ëŒ€ì‹  ì‚¬ìš©)
            # forward hook ëŒ€ì‹  actual forward methodë¥¼ êµì²´í•˜ì—¬ gradient graphê°€ ëŠê¸°ì§€ ì•Šë„ë¡ í•¨
            self.original_forwards = {}
            
            for layer_idx, layer_type in layers_with_basis:
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                # ì›ë³¸ forward ì €ì¥ (tuple keyë¡œ ì €ì¥)
                self.original_forwards[(layer_idx, layer_type)] = target_module.forward
                
                # ìƒˆ forward ë©”ì„œë“œ ìƒì„± (í´ë¡œì €ë¡œ basis_coeffì™€ U_matrix ìº¡ì²˜)
                def make_new_forward(module, orig_forward, layer_idx, layer_type):
                    def new_forward(x):
                        # basis_coeff @ U^T @ x^T
                        if hasattr(module, 'basis_coeff') and hasattr(module, 'U_matrix'):
                            basis_coeff = module.basis_coeff  # (d_out, rank)
                            U_matrix = module.U_matrix        # (d_in, rank)
                            weight_reconstructed = basis_coeff @ U_matrix.T  # (d_out, d_in)
                            # Linear forward: y = x @ W^T + bias
                            return torch.nn.functional.linear(x, weight_reconstructed, module.bias)
                        else:
                            # fallback to original
                            return orig_forward(x)
                    return new_forward
                
                target_module.forward = make_new_forward(target_module, self.original_forwards[(layer_idx, layer_type)], layer_idx, layer_type)
            
            self.logger.info(f"âœ“ Forward ë©”ì„œë“œ {len(layers_with_basis)}ê°œ (layer, type) ì¡°í•©ì—ì„œ êµì²´ë¨ (autograd í˜¸í™˜)")
            
            # âœ… Step 4: Importance ì €ì¥ì†Œ ì´ˆê¸°í™” - Tuple keys for (layer_idx, layer_type)
            importances = {key: [] for key in layers_with_basis}
            
            self.logger.info("\n" + "="*70)
            self.logger.info("Fine-tuning with Importance Tracking")

            self.logger.info("="*70)
            
            # âœ… Step 5: í›ˆë ¨ ë£¨í”„
            epochs = getattr(self.args, 'safety_epochs', 3)
            total_loss = 0.0
            total_batches = 0
            
            for epoch in range(epochs):
                epoch_loss = 0.0
                epoch_batches = 0
                
                progress_bar = tqdm(
                    self.dataloader,
                    desc=f"Epoch {epoch+1}/{epochs}",
                    total=len(self.dataloader)
                )
                
                for batch_idx, batch in enumerate(progress_bar):
                    harmful_prompts = batch['harmful_prompt']
                    safety_responses = batch['safe_response']
                    
                    # ê²°í•©ëœ ì…ë ¥-ëª©í‘œ ì‹œí€€ìŠ¤ (Teacher Forcing)
                    combined_texts = [
                        f"{q}\n{a}" 
                        for q, a in zip(harmful_prompts, safety_responses)
                    ]
                    
                    # í† í°í™”
                    combined = self.tokenizer(
                        combined_texts,
                        return_tensors='pt',
                        padding=True,
                        truncation=True,
                        max_length=512
                    )
                    combined_ids = combined['input_ids'].to(self.model.device)
                    combined_attn = combined['attention_mask'].to(self.model.device)
                    
                    # âœ… Forward pass: weight = basis_coeff @ U^T
                    outputs = self.model(
                        input_ids=combined_ids,
                        attention_mask=combined_attn
                    )
                    logits = outputs.logits  # (batch, seq_len, vocab_size)
                    
                    # Teacher forcing: shift targets
                    pred_logits = logits[:, :-1, :].contiguous()  # (batch, seq_len-1, vocab_size)
                    target_ids_shift = combined_ids[:, 1:].contiguous()
                    attention_mask_shift = combined_attn[:, 1:].contiguous()
                    
                    # ìœ íš¨í•œ í† í°ë§Œ
                    valid_mask = (attention_mask_shift == 1) & (target_ids_shift != self.tokenizer.pad_token_id)
                    pred_logits_flat = pred_logits[valid_mask]
                    target_ids_flat = target_ids_shift[valid_mask]
                    
                    if len(target_ids_flat) > 0:
                        # âœ… Loss ê³„ì‚°
                        loss = nn.CrossEntropyLoss()(pred_logits_flat, target_ids_flat)
                        
                        # âœ… Backward: basis_coeff.grad ê³„ì‚°
                        optimizer.zero_grad()
                        loss.backward()
                        
                        # âœ… Importance ìˆ˜ì§‘: |basis_coeff.grad| - Multiple Layer Types
                        batch_importance_collected = 0
                        for layer_idx, layer_type in layers_with_basis:
                            layer = self.model.model.layers[layer_idx]
                            target_module = self._get_target_module(layer, layer_type)
                            
                            if hasattr(target_module, 'basis_coeff'):
                                if target_module.basis_coeff.grad is not None:
                                    # Gradient ì ˆëŒ“ê°’ (element-wise)
                                    grad_abs = torch.abs(target_module.basis_coeff.grad)  # (d_out, rank)
                                    importances[(layer_idx, layer_type)].append(grad_abs.detach().cpu())
                                    batch_importance_collected += 1
                                else:
                                    self.logger.debug(f"[Batch {batch_idx}] Layer {layer_idx} ({layer_type}): gradient is None!")
                            else:
                                self.logger.debug(f"[Batch {batch_idx}] Layer {layer_idx} ({layer_type}): no basis_coeff!")
                        
                        # âœ… Update: basis_coeff ì—…ë°ì´íŠ¸
                        optimizer.step()
                        
                        epoch_loss += loss.item()
                        epoch_batches += 1
                        total_loss += loss.item()
                        total_batches += 1
                        
                        progress_bar.set_postfix({'loss': loss.item()})
                    
                    progress_bar.update(1)
                
                epoch_loss_avg = epoch_loss / max(epoch_batches, 1)
                self.logger.info(f"\n[Epoch {epoch+1}/{epochs}] Average Loss: {epoch_loss_avg:.4f}")
            
            # âœ… í›ˆë ¨ ì™„ë£Œ í›„ forward ë©”ì„œë“œëŠ” ë³µì›í•˜ì§€ ì•ŠìŒ
            # ì´ìœ : basis_coeffëŠ” ì´ë¯¸ íŒŒì¸íŠœë‹ë¨ (í›ˆë ¨ ì¤‘ ì—…ë°ì´íŠ¸ë¨)
            #      forwardë¥¼ ë³µì›í•  í•„ìš” ì—†ìŒ (basis_coeff @ U^Të¥¼ ê³„ì† ì‚¬ìš©í•´ì•¼ í•¨)
            #      ëŒ€ì‹  save_finetuned_model()ì—ì„œ weight.dataì— ì§ì ‘ ì €ì¥
            self.logger.info(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
            self.logger.info(f"   - basis_coeff: í›ˆë ¨ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨")
            self.logger.info(f"   - Forward ë©”ì„œë“œ: new_forward ìœ ì§€ (basis_coeff @ U^T ì‚¬ìš©)")
            self.logger.info(f"   - ë‹¤ìŒ: importance ì§‘ê³„ ë° ë§ˆìŠ¤í¬ ìƒì„±")
            
            # âœ… Step 6: Importance í‰ê·  ê³„ì‚° (íŒŒì¸íŠœë‹ ì¤‘ ìˆ˜ì§‘í•œ gradient ê¸°ë°˜)
            self.logger.info("\n" + "="*70)
            self.logger.info("ğŸ“Š Importance Scores ê³„ì‚° (ê¸°ì € ê³µê°„ì—ì„œ ìˆ˜ì§‘í•œ gradient ê¸°ë°˜)")
            self.logger.info("="*70)
            self.logger.info("ìˆ˜ì‹: importance_per_input = mean(|âˆ‚L/âˆ‚basis_coeff|) over batches")
            self.logger.info("ì˜ë¯¸: ì•ˆì „ íŒŒì¸íŠœë‹ ì¤‘ ê° ê¸°ì € ì°¨ì›ì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í–ˆëŠ”ê°€?")
            self.logger.info("="*70)
            
            self.importances = {}
            for layer_idx, layer_type in layers_with_basis:
                if len(importances[(layer_idx, layer_type)]) > 0:
                    # ëª¨ë“  ë°°ì¹˜ì˜ gradientë¥¼ ìŠ¤íƒ
                    layer_importances = torch.stack(importances[(layer_idx, layer_type)], dim=0)  # (num_batches, d_out, rank)
                    
                    self.logger.info(f"\nâœ“ Layer {layer_idx} ({layer_type}):")
                    self.logger.info(f"  - ìˆ˜ì§‘ëœ gradient ë°°ì¹˜ ìˆ˜: {len(importances[(layer_idx, layer_type)])}")
                    self.logger.info(f"  - ê° ë°°ì¹˜ shape: (d_out, rank) = {layer_importances[0].shape}")
                    self.logger.info(f"  - ìŠ¤íƒ í›„ shape: {layer_importances.shape}")
                    
                    # ë°°ì¹˜ ì¶• í‰ê·  (ê° (out, in) ìœ„ì¹˜ì—ì„œ ë°°ì¹˜ë“¤ì˜ gradient í‰ê· )
                    importance_mean = layer_importances.mean(dim=0)  # (d_out, rank)
                    self.logger.info(f"  - Batch í‰ê·  shape: {importance_mean.shape}")
                    
                    # Input ì°¨ì›ë³„ë¡œ sum
                    # ê° input ì°¨ì›ì´ ëª¨ë“  outputì— ë¯¸ì¹˜ëŠ” ëˆ„ì  ì˜í–¥ë„ ê³„ì‚°
                    importance_per_input = importance_mean.sum(dim=0)  # (rank,)
                    self.logger.info(f"  - Output ì¶• í•©ì‚° â†’ input-wise importance: {importance_per_input.shape}")
                    
                    self.importances[(layer_idx, layer_type)] = importance_per_input.float().cpu().numpy()
                    
                    # ìƒì„¸ í†µê³„
                    self.logger.info(f"  ğŸ“ˆ í†µê³„:")
                    self.logger.info(f"     - Mean: {self.importances[(layer_idx, layer_type)].mean():.6f}")
                    self.logger.info(f"     - Std: {self.importances[(layer_idx, layer_type)].std():.6f}")
                    self.logger.info(f"     - Min: {self.importances[(layer_idx, layer_type)].min():.6f}")
                    self.logger.info(f"     - Max: {self.importances[(layer_idx, layer_type)].max():.6f}")
                    self.logger.info(f"     - Median: {np.median(self.importances[(layer_idx, layer_type)]):.6f}")
                    
                    # ìƒìœ„ 10% ê°’ í™•ì¸
                    top_10_pct = np.percentile(self.importances[(layer_idx, layer_type)], 90)
                    self.logger.info(f"     - 90 percentile (ìƒìœ„ 10% ê¸°ì¤€): {top_10_pct:.6f}")
                    
                else:
                    self.logger.error(f"âœ— Layer {layer_idx} ({layer_type}): No gradients collected! importances[({layer_idx}, {layer_type})] = {importances.get((layer_idx, layer_type), [])}")
            
            avg_loss = total_loss / max(total_batches, 1)
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"âœ… Phase 2 ì™„ë£Œ: Fine-tuning + Importance Scoring")
            self.logger.info(f"{'='*70}")
            self.logger.info(f"ğŸ“Š í›ˆë ¨ ê²°ê³¼:")
            self.logger.info(f"   - Total loss (all epochs): {total_loss:.4f}")
            self.logger.info(f"   - Average loss per batch: {avg_loss:.4f}")
            self.logger.info(f"   - Total batches processed: {total_batches}")
            self.logger.info(f"   - Layers with importance scores: {len(self.importances)}")
            self.logger.info(f"   - Layers with basis: {len(layers_with_basis)}")
            self.logger.info(f"\nğŸ”‘ WaRP ê²€ì¦:")
            self.logger.info(f"   âœ“ Forward: ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„ (W = basis_coeff @ U^T)")
            self.logger.info(f"   âœ“ Backward: ê¸°ì € ê³µê°„ (âˆ‚L/âˆ‚basis_coeff)")
            self.logger.info(f"   âœ“ Importance: íŒŒì¸íŠœë‹ ì¤‘ ìˆ˜ì§‘ë¨")
            self.logger.info(f"   âœ“ basis_coeff: í›ˆë ¨ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨")
            self.logger.info(f"\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
            self.logger.info(f"   1. ë§ˆìŠ¤í¬ ìƒì„± (ìƒìœ„ 10% ê¸°ì € ì°¨ì› ì„ ë³„)")
            self.logger.info(f"   2. ì•ˆì „ ì •ë ¬ ëª¨ë¸ ì €ì¥")
            self.logger.info(f"   3. Phase 3: GSM8Kë¡œ masked fine-tuning")
            self.logger.info(f"{'='*70}\n")
            
            self.stats['total_loss'] = total_loss
            
        except Exception as e:
            self.logger.error(f"Failed to compute importance: {str(e)}", exc_info=True)
            raise
    
    def save_finetuned_model(self):
        """
        ì•ˆì „í•˜ê²Œ fine-tuningëœ ëª¨ë¸ ì €ì¥
        
        ëª©í‘œ: basis_coeff @ U^Të¡œ weightë¥¼ ì¬êµ¬ì„±í•˜ì—¬ ìµœì¢… ëª¨ë¸ ì €ì¥
        
        ì¤‘ìš”: í›ˆë ¨ ì¤‘ ì—…ë°ì´íŠ¸ëœ basis_coeffë¥¼ ì‚¬ìš©!
        
        ì ˆì°¨:
        1. ì—…ë°ì´íŠ¸ëœ basis_coeff @ U^T ê³„ì‚° (í›ˆë ¨ëœ ëª¨ë¸)
        2. weight.dataì— ì¬êµ¬ì„±ëœ ê°€ì¤‘ì¹˜ í• ë‹¹
        3. ëª¨ë¸ì„ HuggingFace í˜•ì‹ìœ¼ë¡œ ì €ì¥
        
        ê²°ê³¼:
        - ì•ˆì „í•˜ê²Œ fine-tuningëœ ëª¨ë¸ì´ ì €ì¥ë¨
        - basis_coeffëŠ” í›ˆë ¨ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨
        - Phase 3ì—ì„œ ì´ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ masked fine-tuning ìˆ˜í–‰
        """
        try:
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"ğŸ’¾ [Step 1] ìµœì¢… ëª¨ë¸ ì¬êµ¬ì„±")
            self.logger.info(f"{'='*70}")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (dropout ë“± ë¹„í™œì„±í™”)
            self.model.eval()
            
            # âœ… Step 1: ê° ë ˆì´ì–´ì˜ weightë¥¼ basis_coeff @ U^Të¡œ ì¬êµ¬ì„± - Multiple Layer Types
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            layers_with_basis = [key for key in self.basis_data.keys() 
                                if key[0] in target_indices]
            
            self.logger.info(f"ì¬êµ¬ì„±í•  (layer, type) ì¡°í•© ìˆ˜: {len(layers_with_basis)}")
            self.logger.info(f"ìˆ˜ì‹: weight_final = basis_coeff_trained @ U^T")
            self.logger.info(f"ì˜ë¯¸: í›ˆë ¨ëœ ê¸°ì € ê³„ìˆ˜ë¥¼ ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„ìœ¼ë¡œ ë³€í™˜")
            
            for layer_idx, layer_type in layers_with_basis:
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                if hasattr(target_module, 'basis_coeff') and hasattr(target_module, 'U_matrix'):
                    self.logger.debug(f"\n  Layer {layer_idx} ({layer_type}) ì²˜ë¦¬ ì¤‘...")
                    
                    # í›ˆë ¨ëœ basis_coeff ì¶”ì¶œ (detach í›„ CPUë¡œ)
                    basis_coeff_trained = target_module.basis_coeff.detach().cpu()  # (d_out, rank)
                    basis_coeff_init = self.basis_coeffs.get((layer_idx, layer_type), None)  # ì´ˆê¸°ê°’
                    U_matrix = target_module.U_matrix.detach().cpu()  # (d_in, rank) - CPUë¡œ
                    
                    self.logger.debug(f"    - basis_coeff shape: {basis_coeff_trained.shape} (í›ˆë ¨ë¨)")
                    self.logger.debug(f"    - U_matrix shape: {U_matrix.shape} (ê³ ì •)")
                    
                    # í›ˆë ¨ ì „í›„ ë¹„êµ
                    if basis_coeff_init is not None:
                        try:
                            # basis_coeff_initì„ CPU tensorë¡œ ë³€í™˜
                            if isinstance(basis_coeff_init, np.ndarray):
                                basis_coeff_init_tensor = torch.from_numpy(basis_coeff_init).float()
                            else:
                                basis_coeff_init_tensor = basis_coeff_init.detach().cpu() if hasattr(basis_coeff_init, 'detach') else torch.tensor(basis_coeff_init).float()
                            
                            coeff_change = (basis_coeff_trained - basis_coeff_init_tensor).norm().item()
                            self.logger.info(f"  âœ“ Layer {layer_idx} ({layer_type}) - basis_coeff ë³€í™”:")
                            self.logger.info(f"     - Frobenius norm of change: {coeff_change:.6f}")
                            self.logger.info(f"     - ì´ˆê¸°ê°’ norm: {basis_coeff_init_tensor.norm().item():.6f}")
                            self.logger.info(f"     - í›ˆë ¨í›„ norm: {basis_coeff_trained.norm().item():.6f}")
                        except Exception as e:
                            self.logger.warning(f"  âš  Layer {layer_idx} ({layer_type}) - ë³€í™” ë¹„êµ ì‹¤íŒ¨: {str(e)}")
                    
                    # ê°€ì¤‘ì¹˜ ì¬êµ¬ì„±: basis_coeff @ U^T
                    weight_reconstructed = basis_coeff_trained @ U_matrix.T  # (d_out, d_in)
                    
                    self.logger.debug(f"    - weight_reconstructed shape: {weight_reconstructed.shape}")
                    
                    # weight.dataì— í• ë‹¹ (GPUë¡œ ì˜®ê¹€)
                    target_module.weight.data = weight_reconstructed.to(target_module.weight.device)
            
            # âœ… Step 2: ëª¨ë¸ì„ transformers í˜•ì‹ìœ¼ë¡œ ì €ì¥
            model_save_dir = os.path.join(self.args.checkpoint_dir, 'phase2_finetuned_model')
            os.makedirs(model_save_dir, exist_ok=True)
            
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"ğŸ’¾ [Step 2] ì•ˆì „ ì •ë ¬ ëª¨ë¸ ì €ì¥")
            self.logger.info(f"{'='*70}")
            
            self.model.save_pretrained(model_save_dir)
            self.tokenizer.save_pretrained(model_save_dir)
            
            self.logger.info(f"âœ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_dir}")
            self.logger.info(f"  - Format: HuggingFace (pytorch_model.bin + tokenizer)")
            self.logger.info(f"  - ì´ ëª¨ë¸ì€ ì•ˆì „ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì…ë‹ˆë‹¤")
            self.logger.info(f"  - Phase 3ì—ì„œ ì´ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ masked fine-tuning ìˆ˜í–‰")
            
            return model_save_dir
            
        except Exception as e:
            self.logger.error(f"Failed to save finetuned model: {str(e)}", exc_info=True)
            raise
    
    def save_basis_coefficients(self):
        """
        í•™ìŠµëœ basis_coeff ì €ì¥ (Phase 3ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
        
        ëª©í‘œ: basis_coeffë¥¼ ì €ì¥í•˜ì—¬ Phase 3ì—ì„œ ë¡œë“œ ê°€ëŠ¥í•˜ê²Œ í•¨
        
        ê²°ê³¼:
        - layer_type ì„œë¸Œë””ë ‰í† ë¦¬ì— basis_coeff_{layer_idx}.pt íŒŒì¼ ìƒì„±
        - Phase 3ì—ì„œ ì´ë¥¼ ë¡œë“œí•˜ì—¬ basis_coeff ì‚¬ìš© ê°€ëŠ¥
        """
        try:
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"[Step 3] Saving Basis Coefficients (Multiple Layer Types)")
            self.logger.info(f"{'='*70}")
            
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            layers_with_basis = [key for key in self.basis_data.keys() 
                                if key[0] in target_indices]
            
            for layer_idx, layer_type in layers_with_basis:
                coeffs_dir = os.path.join(self.args.checkpoint_dir, 'basis_coefficients', layer_type)
                os.makedirs(coeffs_dir, exist_ok=True)
                
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                if hasattr(target_module, 'basis_coeff'):
                    basis_coeff = target_module.basis_coeff.detach().cpu()
                    save_path = os.path.join(coeffs_dir, f'layer_{layer_idx:02d}_basis_coeff.pt')
                    
                    torch.save({
                        'basis_coeff': basis_coeff,
                        'shape': basis_coeff.shape,
                        'layer_idx': layer_idx,
                        'layer_type': layer_type,
                    }, save_path)
                    
                    self.logger.info(f"  âœ“ Layer {layer_idx} ({layer_type}): {basis_coeff.shape} saved")
            
            self.logger.info(f"âœ“ Basis coefficients saved: basis_coefficients/(layer_type)/")
            self.logger.info(f"{'='*70}\n")
            
        except Exception as e:
            self.logger.error(f"Failed to save basis coefficients: {str(e)}", exc_info=True)
            raise
    
    def generate_masks(self, keep_ratio=0.1):
        """
        Importance ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„± (Element-wise) - Multiple Layer Types
        
        ëª©í‘œ: ì•ˆì „ íŒŒì¸íŠœë‹ ì¤‘ ì¤‘ìš”í•œ ê¸°ì € ì°¨ì›ì„ ì„ ë³„í•˜ì—¬ Phase 3ì—ì„œ ë³´í˜¸
        
        ë°©ì‹:
        1. importance ì ìˆ˜ ê¸°ë°˜ threshold ê³„ì‚°
        2. ìƒìœ„ keep_ratio (10%) ì°¨ì›ì„ "ì¤‘ìš”"ë¡œ í‘œì‹œ
        3. ë‚˜ë¨¸ì§€ 90% ì°¨ì›ì€ Phase 3ì—ì„œ í•™ìŠµ ê°€ëŠ¥
        
        ê²°ê³¼:
        - mask[i] = 1 (ë˜ëŠ” True): ì¤‘ìš”í•œ ì°¨ì› â†’ Phase 3ì—ì„œ ë™ê²°
        - mask[i] = 0 (ë˜ëŠ” False): ëœ ì¤‘ìš”í•œ ì°¨ì› â†’ Phase 3ì—ì„œ í•™ìŠµ ê°€ëŠ¥
        
        Args:
            keep_ratio: ìœ ì§€í•  weightì˜ ë¹„ìœ¨ (0.1 = ìƒìœ„ 10%)
        """
        try:
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"ğŸ¯ ë§ˆìŠ¤í¬ ìƒì„± (Element-wise, Multiple Layer Types)")
            self.logger.info(f"{'='*70}")
            self.logger.info(f"ëª©í‘œ: ì•ˆì „ íŒŒì¸íŠœë‹ ì¤‘ ì¤‘ìš”í•œ ê¸°ì € ì°¨ì› ì„ ë³„")
            self.logger.info(f"ë°©ì‹: Quantile ê¸°ë°˜ ìƒìœ„ {int(keep_ratio*100)}% ì„ ë³„")
            
            for (layer_idx, layer_type), importance in self.importances.items():
                self.logger.info(f"\n  Layer {layer_idx} ({layer_type}):")
                
                # í‰íƒ„í™”ëœ importanceì—ì„œ quantile ê¸°ë°˜ threshold ê³„ì‚°
                importance_flat = importance.flatten()
                threshold = np.quantile(importance_flat, 1 - keep_ratio)
                
                self.logger.info(f"    - Importance ë²”ìœ„: [{importance_flat.min():.6f}, {importance_flat.max():.6f}]")
                self.logger.info(f"    - Threshold (ìƒìœ„ {int(keep_ratio*100)}%): {threshold:.6f}")
                
                # ì´ì§„ ë§ˆìŠ¤í¬ ìƒì„± (1: ì¤‘ìš”/ë™ê²°, 0: ëœ ì¤‘ìš”/í•™ìŠµ ê°€ëŠ¥)
                mask = (importance_flat >= threshold).astype(np.float32)
                
                self.masks[(layer_idx, layer_type)] = mask
                
                frozen_count = mask.sum()
                trainable_count = len(mask) - frozen_count
                actual_ratio = frozen_count / len(mask)
                
                self.logger.info(f"    ğŸ“Š ë§ˆìŠ¤í¬ í†µê³„:")
                self.logger.info(f"       - ë™ê²° ì°¨ì› (mask=1): {int(frozen_count)}/{len(mask)} ({actual_ratio*100:.1f}%)")
                self.logger.info(f"       - í•™ìŠµ ê°€ëŠ¥ ì°¨ì› (mask=0): {int(trainable_count)}/{len(mask)} ({(1-actual_ratio)*100:.1f}%)")
                self.logger.info(f"       - Phase 3ì—ì„œ {int(trainable_count)}ê°œ ì°¨ì›ë§Œ ì—…ë°ì´íŠ¸ë¨")
            
            self.logger.info(f"\nâœ“ ë§ˆìŠ¤í¬ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"Failed to generate masks: {str(e)}", exc_info=True)
            raise
    
    def save_masks(self):
        """
        ìƒì„±ëœ ë§ˆìŠ¤í¬ë¥¼ ì €ì¥
        
        ëª©í‘œ: Phase 3ì—ì„œ ì‚¬ìš©í•  ë§ˆìŠ¤í¬ ì €ì¥
        
        íŒŒì¼ êµ¬ì¡°:
        - masks/
          - layer_29_mask.pt
          - layer_30_mask.pt
          - layer_31_mask.pt
          - metadata.json (í†µê³„)
        
        Log:
        - ì €ì¥ ê²½ë¡œ
        - ê° ë ˆì´ì–´ë³„ ë™ê²°/í•™ìŠµ ê°€ëŠ¥ ì°¨ì› ìˆ˜
        """
        try:
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"ğŸ’¾ ë§ˆìŠ¤í¬ ì €ì¥ (Multiple Layer Types)")
            self.logger.info(f"{'='*70}")
            
            # ë§ˆìŠ¤í¬ ì €ì¥ - layer_type ì„œë¸Œë””ë ‰í† ë¦¬ êµ¬ì¡°
            total_frozen = 0
            total_trainable = 0
            
            for (layer_idx, layer_type), mask in self.masks.items():
                masks_dir = os.path.join(self.args.checkpoint_dir, 'masks', layer_type)
                os.makedirs(masks_dir, exist_ok=True)
                
                save_path = os.path.join(masks_dir, f'layer_{layer_idx:02d}_mask.pt')
                torch.save(torch.from_numpy(mask).float(), save_path)
                
                frozen_count = int(mask.sum())
                trainable_count = len(mask) - frozen_count
                
                total_frozen += frozen_count
                total_trainable += trainable_count
                
                self.logger.debug(f"  âœ“ Layer {layer_idx} ({layer_type}): {frozen_count} frozen, {trainable_count} trainable")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_dir = os.path.join(self.args.checkpoint_dir, 'masks')
            os.makedirs(metadata_dir, exist_ok=True)
            
            metadata = {
                'model_name': self.args.model_name,
                'layer_types': self.args.layer_type if isinstance(self.args.layer_type, list) else self.args.layer_type.split(','),
                'num_masks': len(self.masks),
                'safety_samples': self.args.safety_samples,
                'keep_ratio': self.args.keep_ratio if hasattr(self.args, 'keep_ratio') else 0.1,
                'total_loss': self.stats['total_loss'],
                'total_frozen_dims': int(total_frozen),
                'total_trainable_dims': int(total_trainable),
            }
            
            metadata_path = os.path.join(metadata_dir, 'metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=4)
            
            self.logger.info(f"\nâœ… ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ")
            self.logger.info(f"   - ì €ì¥ ê²½ë¡œ: masks/(layer_type)/")
            self.logger.info(f"   - íŒŒì¼ ìˆ˜: {len(self.masks)} mask files + metadata.json")
            self.logger.info(f"\nğŸ“Š ë§ˆìŠ¤í¬ í†µê³„:")
            self.logger.info(f"   - ì´ ë™ê²° ì°¨ì›: {total_frozen}")
            self.logger.info(f"   - ì´ í•™ìŠµ ê°€ëŠ¥ ì°¨ì›: {total_trainable}")
            self.logger.info(f"   - ì „ì²´: {total_frozen + total_trainable}")
            if total_frozen + total_trainable > 0:
                frozen_ratio = 100 * total_frozen / (total_frozen + total_trainable)
                self.logger.info(f"   - ë™ê²° ë¹„ìœ¨: {frozen_ratio:.1f}%")
            
            self.logger.info(f"\nğŸ”’ Phase 3 ì¤€ë¹„:")
            self.logger.info(f"   - {total_trainable:,}ê°œ ì°¨ì›ì€ GSM8Kë¡œ í•™ìŠµ ê°€ëŠ¥")
            self.logger.info(f"   - {total_frozen:,}ê°œ ì°¨ì›ì€ ì•ˆì „ì„±ì„ ìœ„í•´ ë™ê²°")
            self.logger.info(f"{'='*70}\n")
            
        except Exception as e:
            self.logger.error(f"Failed to save masks: {str(e)}", exc_info=True)
            raise
    
    def _get_target_module(self, layer, layer_type=None):
        """
        ì£¼ì–´ì§„ layerì—ì„œ layer_typeì— ë§ëŠ” ëª¨ë“ˆ ë°˜í™˜
        
        Args:
            layer: transformer layer ê°ì²´
            layer_type: 'ffn_down', 'ffn_up', 'attn_q', 'attn_k', 'attn_v'
                       Noneì´ë©´ self.args.layer_type ì‚¬ìš© (ë‹¨ì¼ íƒ€ì… í˜¸í™˜ì„±)
            
        Returns:
            target_module: ì„ íƒëœ projection ëª¨ë“ˆ
        """
        if layer_type is None:
            # ë‹¨ì¼ layer_type í˜¸í™˜ì„±ì„ ìœ„í•´ argsì—ì„œ ì²« ë²ˆì§¸ íƒ€ì… ì‚¬ìš©
            if isinstance(self.args.layer_type, str):
                layer_type = self.args.layer_type.split(',')[0].strip()
            else:
                layer_type = self.args.layer_type[0]
        
        if layer_type == 'ffn_down':
            return layer.mlp.down_proj
        elif layer_type == 'ffn_up':
            return layer.mlp.up_proj
        elif layer_type == 'attn_q':
            return layer.self_attn.q_proj
        elif layer_type == 'attn_k':
            return layer.self_attn.k_proj
        elif layer_type == 'attn_v':
            return layer.self_attn.v_proj
        else:
            raise ValueError(f"Unknown layer type: {layer_type}")
    
    def _parse_target_layers(self, num_layers):
        """íƒ€ê²Ÿ ë ˆì´ì–´ íŒŒì‹± (Phase 1ê³¼ ë™ì¼)"""
        target = self.args.target_layers.strip()
        
        if target == 'all':
            return list(range(num_layers))
        elif target == 'early':
            return list(range(0, min(11, num_layers)))
        elif target == 'middle':
            return list(range(11, min(22, num_layers)))
        elif target == 'late':
            return list(range(22, num_layers))
        elif target == 'last':
            return [num_layers - 1]
        
        if '-' in target:
            try:
                start, end = target.split('-')
                start, end = int(start.strip()), int(end.strip())
                return list(range(start, min(end + 1, num_layers)))
            except ValueError:
                raise ValueError(f"Invalid range format: {target}")
        
        try:
            layer_idx = int(target)
            if 0 <= layer_idx < num_layers:
                return [layer_idx]
            else:
                raise ValueError(f"Invalid layer index: {layer_idx}")
        except ValueError:
            raise ValueError(f"Invalid target_layers format: {target}")
