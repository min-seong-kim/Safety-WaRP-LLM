"""
Phase 2: Importance Scoring
ì•ˆì „ ë°ì´í„°ë¡œë¶€í„° ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ë°©í–¥ì„ ì‹ë³„í•˜ê³  ë§ˆìŠ¤í¬ ìƒì„±
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
from tqdm import tqdm
import logging

logger = logging.getLogger(__name__)


class Phase2ImportanceScorer:
    """
    Phase 2: Importance Scoring + Fine-tuning
    
    ëª©í‘œ: ì•ˆì „ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ë©´ì„œ ë™ì‹œì— ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    
    ì ˆì°¨:
    1. Phase 1ì—ì„œ ê³„ì‚°ëœ basis ë¡œë“œ
    2. ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ basis ê³µê°„ìœ¼ë¡œ ì¬ë§¤ê°œë³€ìˆ˜í™”
       - W_original (ê³ ì •) â†’ basis_coeff (í•™ìŠµ ê°€ëŠ¥)
       - ëª¨ë“  ì—°ì‚°ì€ basis_coeffë¥¼ í†µí•´ ì§„í–‰
    3. ì—¬ëŸ¬ epoch ë™ì•ˆ ì•ˆì „ ë°ì´í„°ë¡œ ë°˜ë³µ:
       a. ëª¨ë¸ ì‹¤í–‰ (teacher forcing)
       b. ì†ì‹¤ ê³„ì‚°: token-level cross-entropy
       c. ì—­ì „íŒŒ: basis_coeff.grad ê³„ì‚°
       d. ì˜µí‹°ë§ˆì´ì €: basis_coeff ì—…ë°ì´íŠ¸
       e. importance ì ìˆ˜ ëˆ„ì : |âˆ‚L/âˆ‚basis_coeff|
    4. ëª¨ë“  ë°°ì¹˜ì˜ importance í‰ê·  ê³„ì‚°
    5. ì„ê³„ê°’ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„± (ìƒìœ„ keep_ratio ìœ ì§€)
    6. ë§ˆìŠ¤í¬ ì €ì¥
    
    í•µì‹¬:
    - basis_coeffëŠ” Parameterë¡œ ë“±ë¡ë˜ì–´ í•™ìŠµë¨
    - U_matrixëŠ” ê³ ì •ë˜ì–´ ìˆìŒ (requires_grad=False)
    - Weight ë³µì›: W_reconstructed = basis_coeff @ U^T (inference ì‹œ)
    """
    
    def __init__(self, args, logger, basis_dir):
        """
        Args:
            args: ì»¤ë§¨ë“œë¼ì¸ ì¸ì
            logger: ë¡œê±° ê°ì²´
            basis_dir: Phase 1ì—ì„œ ì €ì¥ëœ basis ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.args = args
        self.logger = logger
        self.basis_dir = basis_dir
        
        # ëª¨ë¸ ë° ë°ì´í„°
        self.model = None
        self.tokenizer = None
        self.dataloader = None
        
        # Basis ì •ë³´
        self.basis_data = {}  # (layer_idx, layer_type) -> {'U': U, 'S': S, 'Vh': Vh}
        self.basis_metadata = {}
        self.layer_types = []  # ì²˜ë¦¬í•  layer_type ëª©ë¡
        
        # Reparameterized ê°€ì¤‘ì¹˜
        self.original_weights = {}  # (layer_idx, layer_type) -> W_original
        self.basis_coeffs = {}  # (layer_idx, layer_type) -> basis_coeff (trainable)
        
        # Importance ì ìˆ˜
        self.importances = {}  # layer_idx -> importance ì ìˆ˜ ë°°ì—´
        self.masks = {}  # layer_idx -> ì´ì§„ ë§ˆìŠ¤í¬
        
        # í†µê³„
        self.stats = {
            'total_samples': 0,
            'total_tokens': 0,
            'total_loss': 0.0,
        }
    
    def load_basis(self):
        """
        Phase 1ì—ì„œ ì €ì¥ëœ basis ë¡œë“œ (layer_typeë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ ì§€ì›)
        
        Phase 1ì—ì„œ ì—¬ëŸ¬ layer_typeì„ ì²˜ë¦¬í•œ ê²½ìš°:
        basis/
          â”œâ”€â”€ ffn_down/
          â”‚   â”œâ”€â”€ layer_30_svd.pt
          â”‚   â””â”€â”€ layer_31_svd.pt
          â””â”€â”€ ffn_up/
              â”œâ”€â”€ layer_30_svd.pt
              â””â”€â”€ layer_31_svd.pt
        
        Log:
        - ë¡œë“œëœ íŒŒì¼ ìˆ˜
        - ê° ë ˆì´ì–´ì˜ basis í˜•íƒœ
        - ë©”íƒ€ë°ì´í„° ì •ë³´
        """
        try:
            self.logger.info(f"Loading basis from {self.basis_dir}...")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = os.path.join(self.basis_dir, 'metadata.json')
            with open(metadata_path, 'r') as f:
                self.basis_metadata = json.load(f)
            
            self.logger.info(f"âœ“ Metadata loaded:")
            self.logger.info(f"  - Model: {self.basis_metadata.get('model_name')}")
            self.logger.info(f"  - Layer types available: {self.basis_metadata.get('layer_types', ['unknown'])}")
            self.logger.info(f"  - Target layers: {self.basis_metadata.get('target_layers')}")
            
            # Phase 2ëŠ” ì—¬ëŸ¬ layer_typeì„ ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥
            layer_types_str = self.args.layer_type
            layer_types = [lt.strip() for lt in layer_types_str.split(',')]
            self.layer_types = layer_types
            self.logger.info(f"  - Processing layer types: {layer_types}")
            
            # ê° layer_typeë³„ë¡œ basis ë¡œë“œ
            import glob
            total_loaded = 0
            
            for layer_type in layer_types:
                layer_type_dir = os.path.join(self.basis_dir, layer_type)
                
                self.logger.info(f"  - Layer type '{layer_type}': Using new structure")
                search_dir = layer_type_dir
                
                # í•´ë‹¹ layer_typeì˜ ëª¨ë“  layer_*_svd.pt íŒŒì¼ ì°¾ê¸°
                svd_files = sorted(glob.glob(os.path.join(search_dir, 'layer_*_svd.pt')))
                
                if not svd_files:
                    self.logger.warning(f"  No SVD files found in {search_dir} for layer_type={layer_type}")
                    continue
                
                # ê° SVD íŒŒì¼ ë¡œë“œ
                for svd_path in svd_files:
                    # íŒŒì¼ëª…ì—ì„œ ë ˆì´ì–´ ì¸ë±ìŠ¤ ì¶”ì¶œ: layer_31_svd.pt -> 31
                    filename = os.path.basename(svd_path)
                    layer_idx = int(filename.split('_')[1])
                    
                    svd_data = torch.load(svd_path, map_location='cpu')
                    key = (layer_idx, layer_type)
                    self.basis_data[key] = {
                        'U': svd_data['U'].to(self.args.device),
                        'S': svd_data['S'].to(self.args.device),
                        'Vh': svd_data['Vh'].to(self.args.device),
                    }
                    total_loaded += 1
            
            self.logger.info(f"âœ“ Basis loaded: {total_loaded} (layer, type) combinations")
            self.logger.info(f"  - Keys: {sorted(self.basis_data.keys())}")
            
            # ìƒ˜í”Œ ì •ë³´ ì¶œë ¥
            if len(self.basis_data) > 0:
                sample_key = sorted(self.basis_data.keys())[0]
                sample_U = self.basis_data[sample_key]['U']
                self.logger.info(f"  - Sample {sample_key}: U shape = {sample_U.shape}")
            
        except Exception as e:
            self.logger.error(f"Failed to load basis: {str(e)}", exc_info=True)
            raise
    
    def load_model(self):
        """
        ëª¨ë¸ ë¡œë“œ
        
        Log:
        - ëª¨ë¸ ë¡œë“œ ìƒíƒœ
        - ëª¨ë¸ ì •ë³´
        """
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        try:
            self.logger.info(f"Loading model: {self.args.model_name}")
            
            # ë°ì´í„° íƒ€ì… ì„¤ì •
            dtype_map = {
                'float32': torch.float32,
                'float16': torch.float16,
                'bfloat16': torch.bfloat16
            }
            torch_dtype = dtype_map.get(self.args.dtype, torch.bfloat16)
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.args.model_name,
                torch_dtype=torch_dtype,
                device_map=self.args.device,
                trust_remote_code=True
            )
            
            self.logger.info(f"âœ“ Model loaded successfully")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.args.model_name,
                trust_remote_code=True
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.logger.info(f"âœ“ Tokenizer loaded successfully")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}", exc_info=True)
            raise
    
    def load_safety_data(self):
        """
        ì•ˆì „ ë°ì´í„° ë¡œë“œ (circuit_breakers_train.json)
        
        Log:
        - ë°ì´í„°ì…‹ ë¡œë“œ ìƒíƒœ
        - ë°°ì¹˜ ì •ë³´
        """
        import json
        
        try:
            circuit_breakers_path = self.args.circuit_breakers_path
            self.logger.info(f"Loading circuit_breakers data from {circuit_breakers_path}...")
            
            with open(circuit_breakers_path, 'r', encoding='utf-8') as f:
                circuit_breakers_data = json.load(f)
            
            # ìƒ˜í”Œ ìˆ˜ ì œí•œ
            if self.args.circuit_breakers_samples > 0:
                circuit_breakers_data = circuit_breakers_data[:self.args.circuit_breakers_samples]
            
            self.logger.info(f"âœ“ Loaded {len(circuit_breakers_data)} circuit_breakers samples")
            
            # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
            class CircuitBreakersDataset(torch.utils.data.Dataset):
                def __init__(self, data, tokenizer, max_length=512):
                    self.data = data
                    self.tokenizer = tokenizer
                    self.max_length = max_length
                
                def __len__(self):
                    return len(self.data)
                
                def __getitem__(self, idx):
                    sample = self.data[idx]
                    # prompt + llama3_output ê²°í•© (ì•ˆì „í•œ ê±°ë¶€ ì‘ë‹µ)
                    text = f"{sample['prompt']} {sample['llama3_output']}"
                    
                    encoding = self.tokenizer(
                        text,
                        truncation=True,
                        max_length=self.max_length,
                        return_tensors='pt'
                    )
                    
                    return {
                        'input_ids': encoding['input_ids'].squeeze(),
                        'attention_mask': encoding['attention_mask'].squeeze(),
                    }
            
            dataset = CircuitBreakersDataset(circuit_breakers_data, self.tokenizer, max_length=512)
            
            # Custom collate function
            def collate_fn(batch):
                max_len = max(len(item['input_ids']) for item in batch)
                
                input_ids_list = []
                attention_masks_list = []
                
                for item in batch:
                    input_ids = item['input_ids']
                    attn_mask = item['attention_mask']
                    
                    pad_len = max_len - len(input_ids)
                    if pad_len > 0:
                        input_ids = torch.nn.functional.pad(
                            input_ids.unsqueeze(0),
                            (0, pad_len),
                            value=self.tokenizer.pad_token_id
                        ).squeeze(0)
                        attn_mask = torch.nn.functional.pad(
                            attn_mask.unsqueeze(0),
                            (0, pad_len),
                            value=0
                        ).squeeze(0)
                    
                    input_ids_list.append(input_ids)
                    attention_masks_list.append(attn_mask)
                
                return {
                    'input_ids': torch.stack(input_ids_list),
                    'attention_mask': torch.stack(attention_masks_list),
                }
            
            self.dataloader = torch.utils.data.DataLoader(
                dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                collate_fn=collate_fn
            )
            
            self.logger.info(f"âœ“ Dataloader created")
            self.logger.info(f"  - Batch size: {self.args.batch_size}")
            self.logger.info(f"  - Total batches: {len(self.dataloader)}")
            
        except Exception as e:
            self.logger.error(f"Failed to load safety data: {str(e)}", exc_info=True)
            raise
    
    def reparameterize_weights(self):
        """
        ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ basis ê³µê°„ìœ¼ë¡œ ì¬ë§¤ê°œë³€ìˆ˜í™”
        

        ê´€ê³„ì‹: W_reconstructed = basis_coeff @ U^T                
        
        ë‹¨ê³„:
        1. ì›ë³¸ W ì €ì¥ (ê³ ì •)
        2. basis_coeff ì´ˆê¸°í™” (Wë¥¼ basisë¡œ íˆ¬ì˜) â†’ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡
        3. U_matrix ì €ì¥ (ê³ ì •)
        4. Forward passì—ì„œ weightë¥¼ basis_coeff @ U^Të¡œ ë™ì  ë³µì›
        
        Log:
        - ì¬ë§¤ê°œë³€ìˆ˜í™”ëœ ë ˆì´ì–´ ìˆ˜
        - ê° ë ˆì´ì–´ì˜ í˜•íƒœ
        """
        try:
            self.logger.info("Reparameterizing weights to basis space...")
            self.logger.info("Weight Space â†’ Basis Space Transformation (Multiple Layer Types)")
            self.logger.info("="*70)
            
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            
            # ëª¨ë“  (layer_idx, layer_type) ì¡°í•©ì— ëŒ€í•´ reparameterize
            for layer_idx in target_indices:
                layer = self.model.model.layers[layer_idx]
                
                for layer_type in self.layer_types:
                    key = (layer_idx, layer_type)
                    
                    if key not in self.basis_data:
                        self.logger.debug(f"Layer {layer_idx} ({layer_type}): No basis available, skipping")
                        continue
                    
                    # Select target module based on layer_type
                    if layer_type == 'ffn_down':
                        target_module = layer.mlp.down_proj
                    elif layer_type == 'ffn_up':
                        target_module = layer.mlp.up_proj
                    elif layer_type == 'attn_q':
                        target_module = layer.self_attn.q_proj
                    elif layer_type == 'attn_k':
                        target_module = layer.self_attn.k_proj
                    elif layer_type == 'attn_v':
                        target_module = layer.self_attn.v_proj
                    else:
                        raise ValueError(f"Unknown layer type: {layer_type}")
                    
                    # Step 1: ì›ë³¸ ê°€ì¤‘ì¹˜ ì €ì¥ (ë¶„ì„ìš©, ê³ ì •)
                    W_original = target_module.weight.data.clone()
                    self.original_weights[key] = W_original
                    
                    # Step 2: Basis í–‰ë ¬ ì¶”ì¶œ ë° dtype ë³€í™˜
                    VT_forward = self.basis_data[key]['Vh']
                    
                    model_dtype = W_original.dtype
                    VT_forward = VT_forward.to(dtype=model_dtype, device=W_original.device)
                    
                    # Step 3: basis_coeff ì´ˆê¸°í™”
                    basis_coeff_init = W_original @ VT_forward.t()
                    
                    # basis_coeffë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ Parameterë¡œ ë“±ë¡
                    # ì£¼ì˜: ê°™ì€ moduleì„ ì—¬ëŸ¬ layer_typeì—ì„œ ì¬ì‚¬ìš©í•˜ë©´ ì•ˆë˜ë¯€ë¡œ, 
                    # ì¶”ê°€ ì†ì„±ìœ¼ë¡œ ì €ì¥ (ë‚˜ì¤‘ì— ì°¸ì¡°ìš©)
                    target_module.basis_coeff = nn.Parameter(basis_coeff_init.clone(), requires_grad=True)
                    target_module.VT_forward = VT_forward.clone().detach()  # Vh matrix for reconstruction
                    target_module.VT_forward.requires_grad = False
                    
                    self.basis_coeffs[key] = basis_coeff_init
                    
                    # ë¡œê¹…
                    self.logger.info(f"Layer {layer_idx} ({layer_type}):")
                    self.logger.info(f"  âœ“ W_original (ê³ ì •):     {W_original.shape}")
                    self.logger.info(f"  âœ“ basis_coeff (í•™ìŠµ):    {basis_coeff_init.shape}")
                    self.logger.info(f"  âœ“ VT_forward (ê³ ì •):     {VT_forward.shape} (Vh matrix)")
                    self.logger.info(f"  âœ“ Forward: W = basis_coeff @ VT_forward")
            
            self.logger.info(f"{'='*70}")
            self.logger.info(f"âœ“ Reparameterization completed: {len(self.basis_coeffs)} (layer, type) combinations")

            self.logger.info(f"{'='*70}")
            
        except Exception as e:
            self.logger.error(f"Failed to reparameterize weights: {str(e)}", exc_info=True)
            raise
    
    def compute_importance(self):
        """
        ì•ˆì „ ë°ì´í„°ë¡œ fine-tuningí•˜ë©´ì„œ importance ì ìˆ˜ ê³„ì‚°
        
        ê³¼ì •:
        1. ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        2. Optimizer ì„¤ì • (basis_coeff íŒŒë¼ë¯¸í„°ë§Œ)
        3. Forward pass: weight = basis_coeff @ Vhë¡œ ë™ì  ë³µì› (ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„)
        4. Loss ê³„ì‚° ë° ì—­ì „íŒŒ: âˆ‚L/âˆ‚basis_coeff ê³„ì‚° (ê¸°ì € ê³µê°„ì—ì„œ)
        5. Importance ìˆ˜ì§‘: |âˆ‚L/âˆ‚basis_coeff| â† íŒŒì¸íŠœë‹ ê³¼ì • ì¤‘ ìˆ˜ì§‘
        6. Optimizer.step(): basis_coeff ì—…ë°ì´íŠ¸ (ê¸°ì € ê³µê°„ì—ì„œ)
        7. ì—í¬í¬ ì™„ë£Œ í›„ importance í‰ê·  ê³„ì‚°
        
        í•µì‹¬:
        - Forward: ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„ (W = basis_coeff @ Vh)
        - Backward: ê¸°ì € ê³µê°„ (gradientëŠ” basis_coeffì— ëŒ€í•´)
        - Importance: ê¸°ì € ê³µê°„ì—ì„œ ìˆ˜ì§‘ (íŒŒì¸íŠœë‹ê³¼ ë™ì‹œ)

        """
        try:
            self.logger.info("Starting Phase 2: Fine-tuning + Importance Scoring...")
            self.logger.info("="*70)
            self.logger.info("Forward: ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„ (W = basis_coeff @ Vh)")
            self.logger.info("Backward: ê¸°ì € ê³µê°„ (âˆ‚L/âˆ‚basis_coeff ê³„ì‚°)")
            self.logger.info("Importance: íŒŒì¸íŠœë‹ ê³¼ì • ì¤‘ ìˆ˜ì§‘")
            self.logger.info("Training Setup")
            self.logger.info("="*70)
            
            # Step 0: ëª¨ë“  parameterë¥¼ requires_grad=Falseë¡œ ì„¤ì •
            # WaRP layerê°€ ì•„ë‹Œ ë‚˜ë¨¸ì§€ëŠ” gradient ê³„ì‚° ë¶ˆí•„ìš”
            self.logger.info("Step 0: Freeze ëª¨ë“  parameter (WaRP layer ì œì™¸)")
            self.logger.info("="*70)
            
            for param in self.model.parameters():
                param.requires_grad = False
            
            # Step 1: ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë“± í™œì„±í™”)
            self.model.train()
            self.logger.info("âœ“ Model set to training mode")
            
            # Step 2: Optimizer ì„¤ì • (basis_coeff íŒŒë¼ë¯¸í„°ë§Œ) - Multiple Layer Types
            basis_params = []
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            # (layer_idx, layer_type) íŠœí”Œë¡œ filter
            layers_with_basis = [key for key in self.basis_data.keys() 
                                if key[0] in target_indices]
            
            self.logger.info(f"Target indices: {target_indices}")
            self.logger.info(f"Layers in basis_data: {sorted(self.basis_data.keys())}")
            self.logger.info(f"Layers with basis (intersection): {sorted(layers_with_basis)}")
            
            for layer_idx, layer_type in layers_with_basis:
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                self.logger.info(f"[DEBUG] Processing Layer {layer_idx} ({layer_type})")
                self.logger.info(f"  - hasattr(target_module, 'basis_coeff'): {hasattr(target_module, 'basis_coeff')}")
                
                # basis_coeff ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
                if not hasattr(target_module, 'basis_coeff'):
                    self.logger.info(f"  â†’ Creating new basis_coeff for Layer {layer_idx} ({layer_type})")
                    basis_info = self.basis_data[(layer_idx, layer_type)]
                    VT_forward = basis_info['Vh']  # (rank, d_out)
                    
                    # ì˜¬ë°”ë¥¸ ì´ˆê¸°í™”: W_original @ Vh.T
                    # ì´ë ‡ê²Œ í•˜ë©´:
                    #   W_reconstructed = basis_coeff @ Vh
                    #                  = (W_original @ Vh.T) @ Vh
                    #                  â‰ˆ W_original (VhëŠ” orthonormalì´ë¯€ë¡œ)
                    W_original = target_module.weight.data.clone()
                    
                    # dtype ë§ì¶”ê¸°
                    VT_forward_dtype = VT_forward.to(dtype=W_original.dtype, device=W_original.device)
                    
                    # basis_coeff = W @ Vh.T (íˆ¬ì˜)
                    basis_coeff_init = W_original @ VT_forward_dtype.t()
                    
                    basis_coeff = torch.nn.Parameter(basis_coeff_init.clone())
                    target_module.basis_coeff = basis_coeff
                    
                    # WaRP layerì˜ basis_coeffë§Œ requires_grad=Trueë¡œ ì„¤ì •
                    target_module.basis_coeff.requires_grad_(True)
                    
                    # ì›ë³¸ weightëŠ” requires_grad=False (ì´ë¯¸ Step 0ì—ì„œ ì²˜ë¦¬ë¨)
                    target_module.weight.requires_grad_(False)
                    if target_module.bias is not None:
                        target_module.bias.requires_grad_(False)
                    
                    # VT_forwardë„ ì €ì¥ (forwardì—ì„œ ì‚¬ìš©) - requires_grad=False
                    target_module.VT_forward = VT_forward_dtype
                    
                    self.logger.info(f"Layer {layer_idx} ({layer_type}): basis_coeff created (shape: {basis_coeff.shape})")
                
                # basis_coeffë¥¼ optimizerì— ì¶”ê°€
                if hasattr(target_module, 'basis_coeff'):
                    # requires_grad=True í™•ì¸
                    if not target_module.basis_coeff.requires_grad:
                        target_module.basis_coeff.requires_grad_(True)
                    
                    basis_params.append(target_module.basis_coeff)
                    self.logger.info(f"[Phase2] Layer {layer_idx} ({layer_type}): basis_coeff added to optimizer (Vh-based)")
                    self.logger.info(f"         - basis_coeff.requires_grad={target_module.basis_coeff.requires_grad}")
                    self.logger.info(f"         - weight.requires_grad={target_module.weight.requires_grad}")
                    self.logger.info(f"         - bias.requires_grad={target_module.bias.requires_grad if target_module.bias is not None else 'N/A'}")
            
            if len(basis_params) == 0:
                self.logger.error("No basis_coeff parameters found! Skipping importance computation.")
                self.logger.error(f"  - layers_with_basis: {layers_with_basis}")
                self.logger.error(f"  - basis_data keys: {sorted(self.basis_data.keys())}")
                return
            
            learning_rate = getattr(self.args, 'safety_lr', 1e-5)
            weight_decay = getattr(self.args, 'safety_weight_decay', 0.01)
            
            optimizer = torch.optim.AdamW(basis_params, lr=learning_rate, weight_decay=weight_decay)
            
            self.logger.info(f"âœ“ Optimizer ìƒì„± ì™„ë£Œ: AdamW")
            self.logger.info(f"  - Learning rate: {learning_rate}")
            self.logger.info(f"  - Weight decay: {weight_decay}")
            self.logger.info(f"  - ì—…ë°ì´íŠ¸í•  íŒŒë¼ë¯¸í„°: {len(basis_params)} basis_coeff tensors (ì´ {sum(p.numel() for p in basis_params):,}ê°œ íŒŒë¼ë¯¸í„°)")
            self.logger.info(f"  - ì—…ë°ì´íŠ¸í•  ë ˆì´ì–´: {layers_with_basis}")
            
            # ê²€ì¦: í˜„ì¬ requires_grad ìƒíƒœ í™•ì¸
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            frozen_params = total_params - trainable_params
            
            self.logger.info(f"{'='*70}")
            self.logger.info(f"íŒŒë¼ë¯¸í„° ìƒíƒœ ê²€ì¦")
            self.logger.info(f"  - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
            self.logger.info(f"  - ì—…ë°ì´íŠ¸ ëŒ€ìƒ (requires_grad=True): {trainable_params:,} ({100*trainable_params/total_params:.3f}%)")
            self.logger.info(f"  - ë™ê²°ë¨ (requires_grad=False): {frozen_params:,} ({100*frozen_params/total_params:.3f}%)")
            self.logger.info(f"  WaRP layerì˜ basis_coeffë§Œ ì—…ë°ì´íŠ¸ë¨")
            self.logger.info(f"  ë‚˜ë¨¸ì§€ ëª¨ë“  weightëŠ” ë™ê²°ë¨")
            self.logger.info(f"{'='*70}")
            
            # Step 3: Forward ë©”ì„œë“œ êµì²´ - autograd í˜¸í™˜ (hook ëŒ€ì‹  ì‚¬ìš©)
            # forward hook ëŒ€ì‹  actual forward methodë¥¼ êµì²´í•˜ì—¬ gradient graphê°€ ëŠê¸°ì§€ ì•Šë„ë¡ í•¨
            self.original_forwards = {}
            
            for layer_idx, layer_type in layers_with_basis:
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                # ì›ë³¸ forward ì €ì¥ (tuple keyë¡œ ì €ì¥)
                self.original_forwards[(layer_idx, layer_type)] = target_module.forward
                
                # ìƒˆ forward ë©”ì„œë“œ ìƒì„± (í´ë¡œì €ë¡œ basis_coeffì™€ VT_forward ìº¡ì²˜)
                def make_new_forward(module, orig_forward, layer_idx, layer_type):
                    def new_forward(x):
                        # basis_coeff @ Vh
                        if hasattr(module, 'basis_coeff') and hasattr(module, 'VT_forward'):
                            basis_coeff = module.basis_coeff    # (d_out, rank)
                            VT_forward = module.VT_forward      # (rank, d_out)
                            weight_reconstructed = basis_coeff @ VT_forward  # (d_out, d_in)
                            # Linear forward: y = x @ W^T + bias
                            return torch.nn.functional.linear(x, weight_reconstructed, module.bias)
                        else:
                            # fallback to original
                            return orig_forward(x)
                    return new_forward
                
                target_module.forward = make_new_forward(target_module, self.original_forwards[(layer_idx, layer_type)], layer_idx, layer_type)
            
            self.logger.info(f"Forward ë©”ì„œë“œ {len(layers_with_basis)}ê°œ (layer, type) ì¡°í•©ì—ì„œ êµì²´ë¨ (Vh-based autograd í˜¸í™˜)")
            
            # Step 4: Importance ì €ì¥ì†Œ ì´ˆê¸°í™” (Online Averaging - ë©”ëª¨ë¦¬ ìµœì í™”)
            # ê° ë°°ì¹˜ì˜ gradientë¥¼ ì €ì¥í•˜ì§€ ì•Šê³ , ì‹¤ì‹œê°„ìœ¼ë¡œ í‰ê·  ê³„ì‚°
            importance_sum = {key: None for key in layers_with_basis}  # ëˆ„ì  í•©ê³„
            importance_count = {key: 0 for key in layers_with_basis}   # ë°°ì¹˜ ê°œìˆ˜
            
            self.logger.info(f"{'='*70}")
            self.logger.info("Fine-tuning with Online Importance Averaging (Memory-Efficient)")
            self.logger.info("ë©”ëª¨ë¦¬ ìµœì í™”: Gradientë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  ì‹¤ì‹œê°„ í‰ê·  ê³„ì‚°")
            self.logger.info(f"{'='*70}")
            
            # Step 5: í›ˆë ¨ ë£¨í”„ (ë™ì‹œì— importance ê³„ì‚°)
            epochs = getattr(self.args, 'safety_epochs', 3)
            total_loss = 0.0
            total_batches = 0
            
            for epoch in range(epochs):
                epoch_loss = 0.0
                epoch_batches = 0
                
                progress_bar = tqdm(
                    self.dataloader,
                    desc=f"Epoch {epoch+1}/{epochs}",
                    total=len(self.dataloader)
                )
                
                for batch_idx, batch in enumerate(progress_bar):
                    # CircuitBreakersDatasetì—ì„œ ì´ë¯¸ tokenizeëœ input_idsì™€ attention_mask ì‚¬ìš©
                    input_ids = batch['input_ids'].to(self.model.device)
                    attention_mask = batch['attention_mask'].to(self.model.device)
                    
                    # Forward pass: weight = basis_coeff @ Vh
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask
                    )
                    logits = outputs.logits  # (batch, seq_len, vocab_size)
                    
                    # Teacher forcing: shift targets
                    pred_logits = logits[:, :-1, :].contiguous()  # (batch, seq_len-1, vocab_size)
                    target_ids_shift = input_ids[:, 1:].contiguous()
                    attention_mask_shift = attention_mask[:, 1:].contiguous()
                    
                    # ìœ íš¨í•œ í† í°ë§Œ
                    valid_mask = (attention_mask_shift == 1) & (target_ids_shift != self.tokenizer.pad_token_id)
                    pred_logits_flat = pred_logits[valid_mask]
                    target_ids_flat = target_ids_shift[valid_mask]
                    
                    if len(target_ids_flat) > 0:
                        # Loss ê³„ì‚°
                        loss = nn.CrossEntropyLoss()(pred_logits_flat, target_ids_flat)
                        
                        # Backward: basis_coeff.grad ê³„ì‚°
                        optimizer.zero_grad()
                        loss.backward()
                        
                        # Online Importance Averaging: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ í‰ê·  ê³„ì‚° (ë©”ëª¨ë¦¬ ìµœì í™”)
                        for layer_idx, layer_type in layers_with_basis:
                            layer = self.model.model.layers[layer_idx]
                            target_module = self._get_target_module(layer, layer_type)
                            
                            if hasattr(target_module, 'basis_coeff'):
                                if target_module.basis_coeff.grad is not None:
                                    # Gradient ì ˆëŒ“ê°’ (element-wise)
                                    grad_abs = torch.abs(target_module.basis_coeff.grad).float().cpu()  # (d_out, rank)
                                    
                                    # Online í‰ê·  ê³„ì‚°: new_mean = (old_mean * count + grad) / (count + 1)
                                    key = (layer_idx, layer_type)
                                    if importance_sum[key] is None:
                                        # ì²« ë²ˆì§¸ ë°°ì¹˜
                                        importance_sum[key] = grad_abs.clone()
                                    else:
                                        # ì´í›„ ë°°ì¹˜ë“¤: ì˜¨ë¼ì¸ ëˆ„ì 
                                        importance_sum[key] += grad_abs
                                    
                                    importance_count[key] += 1
                                else:
                                    self.logger.debug(f"[Batch {batch_idx}] Layer {layer_idx} ({layer_type}): gradient is None!")
                            else:
                                self.logger.debug(f"[Batch {batch_idx}] Layer {layer_idx} ({layer_type}): no basis_coeff!")
                        
                        # Update: basis_coeff ì—…ë°ì´íŠ¸
                        optimizer.step()
                        
                        epoch_loss += loss.item()
                        epoch_batches += 1
                        total_loss += loss.item()
                        total_batches += 1
                        
                        progress_bar.set_postfix({'loss': loss.item()})
                    
                    progress_bar.update(1)
                
                epoch_loss_avg = epoch_loss / max(epoch_batches, 1)
                self.logger.info(f"[Epoch {epoch+1}/{epochs}] Average Loss: {epoch_loss_avg:.4f}")
            
            # í›ˆë ¨ ì™„ë£Œ í›„ forward ë©”ì„œë“œëŠ” ë³µì›í•˜ì§€ ì•ŠìŒ
            # ì´ìœ : basis_coeffëŠ” ì´ë¯¸ íŒŒì¸íŠœë‹ë¨ (í›ˆë ¨ ì¤‘ ì—…ë°ì´íŠ¸ë¨)
            #      forwardë¥¼ ë³µì›í•  í•„ìš” ì—†ìŒ (basis_coeff @ Vhë¥¼ ê³„ì† ì‚¬ìš©í•´ì•¼ í•¨)
            #      ëŒ€ì‹  save_finetuned_model()ì—ì„œ weight.dataì— ì§ì ‘ ì €ì¥
            self.logger.info(f"í›ˆë ¨ ì™„ë£Œ!")
            self.logger.info(f"   - basis_coeff: í›ˆë ¨ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨")
            self.logger.info(f"   - Forward ë©”ì„œë“œ: new_forward ìœ ì§€ (basis_coeff @ Vh ì‚¬ìš©)")
            self.logger.info(f"   - ë‹¤ìŒ: Online í‰ê· ì—ì„œ ìµœì¢… importance ê³„ì‚° ë° ë§ˆìŠ¤í¬ ìƒì„±")
            
            # Step 6: Online í‰ê· ì—ì„œ ìµœì¢… importance ê³„ì‚°
            self.logger.info("="*70)
            self.logger.info("Importance Scores ê³„ì‚° (Online Averaging ê²°ê³¼)")
            self.logger.info("ìˆ˜ì‹: importance = accumulated_sum / batch_count")
            self.logger.info("ì˜ë¯¸: ì•ˆì „ íŒŒì¸íŠœë‹ ì¤‘ ê° ê¸°ì € ì°¨ì›ì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í–ˆëŠ”ê°€?")
            
            self.importances = {}
            for layer_idx, layer_type in layers_with_basis:
                key = (layer_idx, layer_type)
                if importance_count[key] > 0:
                    # Online í‰ê·  ê³„ì‚°
                    importance_mean = importance_sum[key] / importance_count[key]  # (d_out, rank)
                    
                    self.logger.info(f"âœ“ Layer {layer_idx} ({layer_type}):")
                    self.logger.info(f"  - ëˆ„ì í•œ ë°°ì¹˜ ìˆ˜: {importance_count[key]}")
                    self.logger.info(f"  - Importance shape: {importance_mean.shape}")
                    
                    # 2D í˜•íƒœ ìœ ì§€ (generate_masksì—ì„œ flattení•˜ì—¬ ì²˜ë¦¬)
                    self.importances[key] = importance_mean.float().numpy()
                    
                    # ìƒì„¸ í†µê³„
                    self.logger.info(f"  ğŸ“ˆ Importances score í†µê³„ (2D: d_out Ã— rank):")
                    self.logger.info(f"     - Shape: {self.importances[key].shape}")
                    self.logger.info(f"     - Mean: {self.importances[key].mean():.6f}")
                    self.logger.info(f"     - Std: {self.importances[key].std():.6f}")
                    self.logger.info(f"     - Min: {self.importances[key].min():.6f}")
                    self.logger.info(f"     - Max: {self.importances[key].max():.6f}")
                    self.logger.info(f"     - Median: {np.median(self.importances[key]):.6f}")
                    
                    # ìƒìœ„ 10% ê°’ í™•ì¸
                    top_10_pct = np.percentile(self.importances[key], 90)
                    self.logger.info(f"     - 90 percentile (ìƒìœ„ 10% ê¸°ì¤€): {top_10_pct:.6f}")
                    
                else:
                    self.logger.error(f"âœ— Layer {layer_idx} ({layer_type}): No gradients collected! count = {importance_count[key]}")
            
            avg_loss = total_loss / max(total_batches, 1)
            self.logger.info(f"{'='*70}")
            self.logger.info(f"Phase 2 ì™„ë£Œ: Fine-tuning + Online Importance Averaging")
            self.logger.info(f"{'='*70}")
            self.logger.info(f"í›ˆë ¨ ê²°ê³¼:")
            self.logger.info(f"   - Total loss (all epochs): {total_loss:.4f}")
            self.logger.info(f"   - Average loss per batch: {avg_loss:.4f}")
            self.logger.info(f"   - Total batches processed: {total_batches}")
            self.logger.info(f"   - Layers with importance scores: {len(self.importances)}")
            self.logger.info(f"   - Layers with basis: {len(layers_with_basis)}")
            self.logger.info(f"   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: Gradient ì €ì¥ ëŒ€ì‹  Online Averaging ì‚¬ìš©")
      
            self.stats['total_loss'] = total_loss
            
        except Exception as e:
            self.logger.error(f"Failed to compute importance: {str(e)}", exc_info=True)
            raise
    
    def save_finetuned_model(self):
        """
        ì•ˆì „í•˜ê²Œ fine-tuningëœ ëª¨ë¸ ì €ì¥
        
        ëª©í‘œ: basis_coeff @ Vhë¡œ weightë¥¼ ì¬êµ¬ì„±í•˜ì—¬ ìµœì¢… ëª¨ë¸ ì €ì¥
        
        ì¤‘ìš”: í›ˆë ¨ ì¤‘ ì—…ë°ì´íŠ¸ëœ basis_coeffë¥¼ ì‚¬ìš©!
        
        ì ˆì°¨:
        1. ì—…ë°ì´íŠ¸ëœ basis_coeff @ Vh ê³„ì‚° (í›ˆë ¨ëœ ëª¨ë¸)
        2. weight.dataì— ì¬êµ¬ì„±ëœ ê°€ì¤‘ì¹˜ í• ë‹¹
        3. ëª¨ë¸ì„ HuggingFace í˜•ì‹ìœ¼ë¡œ ì €ì¥
        
        ê²°ê³¼:
        - ì•ˆì „í•˜ê²Œ fine-tuningëœ ëª¨ë¸ì´ ì €ì¥ë¨
        - basis_coeffëŠ” í›ˆë ¨ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨
        - Phase 3ì—ì„œ ì´ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ masked fine-tuning ìˆ˜í–‰
        """
        try:
            self.logger.info(f"[Step 1] ìµœì¢… ëª¨ë¸ ì¬êµ¬ì„±")
            self.logger.info(f"{'='*70}")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (dropout ë“± ë¹„í™œì„±í™”)
            self.model.eval()
            
            # Step 1: ê° ë ˆì´ì–´ì˜ weightë¥¼ basis_coeff @ Vhë¡œ ì¬êµ¬ì„± - Multiple Layer Types
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            layers_with_basis = [key for key in self.basis_data.keys() 
                                if key[0] in target_indices]
            
            self.logger.info(f"ì¬êµ¬ì„±í•  (layer, type) ì¡°í•© ìˆ˜: {len(layers_with_basis)}")
            # ìˆ˜ì‹: weight_final = basis_coeff_trained @ Vh
            # ì˜ë¯¸: í›ˆë ¨ëœ ê¸°ì € ê³„ìˆ˜ë¥¼ ì›ë³¸ ê°€ì¤‘ì¹˜ ê³µê°„ìœ¼ë¡œ ë³€í™˜
            
            for layer_idx, layer_type in layers_with_basis:
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                if hasattr(target_module, 'basis_coeff') and hasattr(target_module, 'VT_forward'):
                    self.logger.debug(f"Layer {layer_idx} ({layer_type}) ì²˜ë¦¬ ì¤‘...")
                    
                    # í›ˆë ¨ëœ basis_coeff ì¶”ì¶œ (detach í›„ CPUë¡œ)
                    basis_coeff_trained = target_module.basis_coeff.detach().cpu()  # (d_out, rank)
                    basis_coeff_init = self.basis_coeffs.get((layer_idx, layer_type), None)  # ì´ˆê¸°ê°’
                    VT_forward = target_module.VT_forward.detach().cpu()  # (rank, d_out) - CPUë¡œ
                    
                    self.logger.debug(f"    - basis_coeff shape: {basis_coeff_trained.shape} (í›ˆë ¨ë¨)")
                    self.logger.debug(f"    - VT_forward shape: {VT_forward.shape} (ê³ ì •, Vh matrix)")
                    
                    # í›ˆë ¨ ì „í›„ ë¹„êµ
                    if basis_coeff_init is not None:
                        try:
                            # basis_coeff_initì„ CPU tensorë¡œ ë³€í™˜
                            if isinstance(basis_coeff_init, np.ndarray):
                                basis_coeff_init_tensor = torch.from_numpy(basis_coeff_init).float()
                            else:
                                basis_coeff_init_tensor = basis_coeff_init.detach().cpu() if hasattr(basis_coeff_init, 'detach') else torch.tensor(basis_coeff_init).float()
                            
                            coeff_change = (basis_coeff_trained - basis_coeff_init_tensor).norm().item()
                            self.logger.info(f"  âœ“ Layer {layer_idx} ({layer_type}) - basis_coeff ë³€í™”:")
                            self.logger.info(f"     - Frobenius norm of change: {coeff_change:.6f}")
                            self.logger.info(f"     - ì´ˆê¸°ê°’ norm: {basis_coeff_init_tensor.norm().item():.6f}")
                            self.logger.info(f"     - í›ˆë ¨í›„ norm: {basis_coeff_trained.norm().item():.6f}")
                        except Exception as e:
                            self.logger.warning(f"  âš  Layer {layer_idx} ({layer_type}) - ë³€í™” ë¹„êµ ì‹¤íŒ¨: {str(e)}")
                    
                    # ê°€ì¤‘ì¹˜ ì¬êµ¬ì„±: basis_coeff @ Vh
                    weight_reconstructed = basis_coeff_trained @ VT_forward  # (d_out, d_in)
                    
                    self.logger.info(f"    - weight_reconstructed shape: {weight_reconstructed.shape}")
                    
                    # weight.dataì— í• ë‹¹ (GPUë¡œ ì˜®ê¹€)
                    target_module.weight.data = weight_reconstructed.to(target_module.weight.device)
            
            # Step 2: ëª¨ë¸ì„ transformers í˜•ì‹ìœ¼ë¡œ ì €ì¥
            model_save_dir = os.path.join(self.args.checkpoint_dir, 'phase2_finetuned_model')
            os.makedirs(model_save_dir, exist_ok=True)
            
            self.logger.info(f"{'='*70}")
            self.logger.info(f"[Step 2] ì•ˆì „ ì •ë ¬ ëª¨ë¸ ì €ì¥")
            self.logger.info(f"{'='*70}")
            
            self.model.save_pretrained(model_save_dir)
            self.tokenizer.save_pretrained(model_save_dir)
            
            self.logger.info(f"âœ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_dir}")
            
            return model_save_dir
            
        except Exception as e:
            self.logger.error(f"Failed to save finetuned model: {str(e)}", exc_info=True)
            raise
    
    def save_basis_coefficients(self):
        """
        í•™ìŠµëœ basis_coeff ì €ì¥ (Phase 3ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
        
        ëª©í‘œ: basis_coeffë¥¼ ì €ì¥í•˜ì—¬ Phase 3ì—ì„œ ë¡œë“œ ê°€ëŠ¥í•˜ê²Œ í•¨
        
        ê²°ê³¼:
        - layer_type ì„œë¸Œë””ë ‰í† ë¦¬ì— basis_coeff_{layer_idx}.pt íŒŒì¼ ìƒì„±
        - Phase 3ì—ì„œ ì´ë¥¼ ë¡œë“œí•˜ì—¬ basis_coeff ì‚¬ìš© ê°€ëŠ¥
        """
        try:
            self.logger.info(f"{'='*70}")
            self.logger.info(f"[Step 3] Saving Basis Coefficients (Multiple Layer Types)")
            self.logger.info(f"{'='*70}")
            
            target_indices = self._parse_target_layers(len(self.model.model.layers))
            layers_with_basis = [key for key in self.basis_data.keys() 
                                if key[0] in target_indices]
            
            for layer_idx, layer_type in layers_with_basis:
                coeffs_dir = os.path.join(self.args.checkpoint_dir, 'basis_coefficients', layer_type)
                os.makedirs(coeffs_dir, exist_ok=True)
                
                layer = self.model.model.layers[layer_idx]
                target_module = self._get_target_module(layer, layer_type)
                
                if hasattr(target_module, 'basis_coeff'):
                    basis_coeff = target_module.basis_coeff.detach().cpu()
                    save_path = os.path.join(coeffs_dir, f'layer_{layer_idx:02d}_basis_coeff.pt')
                    
                    torch.save({
                        'basis_coeff': basis_coeff,
                        'shape': basis_coeff.shape,
                        'layer_idx': layer_idx,
                        'layer_type': layer_type,
                    }, save_path)
                    
                    self.logger.info(f"  âœ“ Layer {layer_idx} ({layer_type}): {basis_coeff.shape} saved")
            
            self.logger.info(f"âœ“ Basis coefficients saved: basis_coefficients/(layer_type)/")
            self.logger.info(f"{'='*70}")
            
        except Exception as e:
            self.logger.error(f"Failed to save basis coefficients: {str(e)}", exc_info=True)
            raise
    
    def generate_masks(self, keep_ratio=0.1):
        """
        Importance ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„± (Element-wise) - Multiple Layer Types
        
        ëª©í‘œ: ì•ˆì „ íŒŒì¸íŠœë‹ ì¤‘ ì¤‘ìš”í•œ ê¸°ì € ì°¨ì›ì„ ì„ ë³„í•˜ì—¬ Phase 3ì—ì„œ ë³´í˜¸
        
        ë°©ì‹:
        1. importance ì ìˆ˜ ê¸°ë°˜ threshold ê³„ì‚°
        2. ìƒìœ„ keep_ratio (10%) ì°¨ì›ì„ "ì¤‘ìš”"ë¡œ í‘œì‹œ
        3. ë‚˜ë¨¸ì§€ 90% ì°¨ì›ì€ Phase 3ì—ì„œ í•™ìŠµ ê°€ëŠ¥
        
        ê²°ê³¼:
        - mask[i] = 1 (ë˜ëŠ” True): ì¤‘ìš”í•œ ì°¨ì› â†’ Phase 3ì—ì„œ ë™ê²°
        - mask[i] = 0 (ë˜ëŠ” False): ëœ ì¤‘ìš”í•œ ì°¨ì› â†’ Phase 3ì—ì„œ í•™ìŠµ ê°€ëŠ¥
        
        Args:
            keep_ratio: ìœ ì§€í•  weightì˜ ë¹„ìœ¨ (0.1 = ìƒìœ„ 10%)
        """
        try:
            self.logger.info(f"{'='*70}")
            self.logger.info(f"ë§ˆìŠ¤í¬ ìƒì„± (Element-wise, Multiple Layer Types)")
            self.logger.info(f"{'='*70}")
            self.logger.info(f"ëª©í‘œ: ì•ˆì „ íŒŒì¸íŠœë‹ ì¤‘ ì¤‘ìš”í•œ ê¸°ì € ì°¨ì› ì„ ë³„")
            self.logger.info(f"ë°©ì‹: Quantile ê¸°ë°˜ ìƒìœ„ {int(keep_ratio*100)}% ì„ ë³„")
            
            for (layer_idx, layer_type), importance in self.importances.items():
                self.logger.info(f"Layer {layer_idx} ({layer_type}):")
                
                # í‰íƒ„í™”ëœ importanceì—ì„œ quantile ê¸°ë°˜ threshold ê³„ì‚°
                importance_flat = importance.flatten()
                threshold = np.quantile(importance_flat, 1 - keep_ratio)
                
                self.logger.info(f"    - Importance ë²”ìœ„: [{importance_flat.min():.6f}, {importance_flat.max():.6f}]")
                self.logger.info(f"    - Threshold (ìƒìœ„ {int(keep_ratio*100)}%): {threshold:.6f}")
                
                # ì´ì§„ ë§ˆìŠ¤í¬ ìƒì„± (1: ì¤‘ìš”/ë™ê²°, 0: ëœ ì¤‘ìš”/í•™ìŠµ ê°€ëŠ¥)
                mask = (importance_flat >= threshold).astype(np.float32)
                
                self.masks[(layer_idx, layer_type)] = mask
                
                frozen_count = mask.sum()
                trainable_count = len(mask) - frozen_count
                actual_ratio = frozen_count / len(mask)
                
                self.logger.info(f"    ë§ˆìŠ¤í¬ í†µê³„:")
                self.logger.info(f"       - ë™ê²° ì°¨ì› (mask=1): {int(frozen_count)}/{len(mask)} ({actual_ratio*100:.1f}%)")
                self.logger.info(f"       - í•™ìŠµ ê°€ëŠ¥ ì°¨ì› (mask=0): {int(trainable_count)}/{len(mask)} ({(1-actual_ratio)*100:.1f}%)")
                self.logger.info(f"       - Phase 3ì—ì„œ {int(trainable_count)}ê°œ ì°¨ì›ë§Œ ì—…ë°ì´íŠ¸ë¨")
            
            self.logger.info(f"âœ“ ë§ˆìŠ¤í¬ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"Failed to generate masks: {str(e)}", exc_info=True)
            raise
    
    def save_masks(self):
        """
        ìƒì„±ëœ ë§ˆìŠ¤í¬ë¥¼ ì €ì¥
        
        ëª©í‘œ: Phase 3ì—ì„œ ì‚¬ìš©í•  ë§ˆìŠ¤í¬ ì €ì¥
        
        íŒŒì¼ êµ¬ì¡°:
        - masks/
          - layer_29_mask.pt
          - layer_30_mask.pt
          - layer_31_mask.pt
          - metadata.json (í†µê³„)
        
        Log:
        - ì €ì¥ ê²½ë¡œ
        - ê° ë ˆì´ì–´ë³„ ë™ê²°/í•™ìŠµ ê°€ëŠ¥ ì°¨ì› ìˆ˜
        """
        try:
            self.logger.info(f"{'='*70}")
            self.logger.info(f"ë§ˆìŠ¤í¬ ì €ì¥ (Multiple Layer Types)")
            self.logger.info(f"{'='*70}")
            
            # ë§ˆìŠ¤í¬ ì €ì¥ - layer_type ì„œë¸Œë””ë ‰í† ë¦¬ êµ¬ì¡°
            total_frozen = 0
            total_trainable = 0
            
            # Shape ë©”íƒ€ë°ì´í„° ì €ì¥ìš©
            mask_shapes = {}
            
            for (layer_idx, layer_type), mask in self.masks.items():
                masks_dir = os.path.join(self.args.checkpoint_dir, 'masks', layer_type)
                os.makedirs(masks_dir, exist_ok=True)
                
                save_path = os.path.join(masks_dir, f'layer_{layer_idx:02d}_mask.pt')
                
                # importanceì˜ 2D shape ì •ë³´ë¥¼ ì €ì¥
                importance_2d = self.importances[(layer_idx, layer_type)]  # (d_out, rank)
                mask_shape = importance_2d.shape  # (d_out, rank)
                
                # maskì™€ shapeì„ í•¨ê»˜ ì €ì¥
                torch.save({
                    'mask': torch.from_numpy(mask).float(),
                    'shape': mask_shape,  # (d_out, rank)
                }, save_path)
                
                mask_shapes[(layer_idx, layer_type)] = mask_shape
                
                frozen_count = int(mask.sum())
                trainable_count = len(mask) - frozen_count
                
                total_frozen += frozen_count
                total_trainable += trainable_count
                
                self.logger.debug(f"  âœ“ Layer {layer_idx} ({layer_type}): shape {mask_shape}, {frozen_count} frozen, {trainable_count} trainable")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_dir = os.path.join(self.args.checkpoint_dir, 'masks')
            os.makedirs(metadata_dir, exist_ok=True)
            
            metadata = {
                'model_name': self.args.model_name,
                'layer_types': self.args.layer_type if isinstance(self.args.layer_type, list) else self.args.layer_type.split(','),
                'num_masks': len(self.masks),
                'circuit_breakers_path': self.args.circuit_breakers_path,
                'circuit_breakers_samples': self.args.circuit_breakers_samples,
                'keep_ratio': self.args.keep_ratio if hasattr(self.args, 'keep_ratio') else 0.1,
                'total_loss': self.stats['total_loss'],
                'total_frozen_dims': int(total_frozen),
                'total_trainable_dims': int(total_trainable),
            }
            
            metadata_path = os.path.join(metadata_dir, 'metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=4)
            
            self.logger.info(f"ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ")
            self.logger.info(f"   - ì €ì¥ ê²½ë¡œ: masks/(layer_type)/")
            self.logger.info(f"   - íŒŒì¼ ìˆ˜: {len(self.masks)} mask files + metadata.json")
            self.logger.info(f"ë§ˆìŠ¤í¬ í†µê³„:")
            self.logger.info(f"   - ì´ ë™ê²° ì°¨ì›: {total_frozen}")
            self.logger.info(f"   - ì´ í•™ìŠµ ê°€ëŠ¥ ì°¨ì›: {total_trainable}")
            self.logger.info(f"   - ì „ì²´: {total_frozen + total_trainable}")
            if total_frozen + total_trainable > 0:
                frozen_ratio = 100 * total_frozen / (total_frozen + total_trainable)
                self.logger.info(f"   - ë™ê²° ë¹„ìœ¨: {frozen_ratio:.1f}%")
            
            self.logger.info(f"Phase 3 ì¤€ë¹„:")
            self.logger.info(f"   - {total_trainable:,}ê°œ ì°¨ì›ì€ GSM8Kë¡œ í•™ìŠµ ê°€ëŠ¥")
            self.logger.info(f"   - {total_frozen:,}ê°œ ì°¨ì›ì€ ì•ˆì „ì„±ì„ ìœ„í•´ ë™ê²°")
            self.logger.info(f"{'='*70}")
            
        except Exception as e:
            self.logger.error(f"Failed to save masks: {str(e)}", exc_info=True)
            raise
    
    def _get_target_module(self, layer, layer_type=None):
        """
        ì£¼ì–´ì§„ layerì—ì„œ layer_typeì— ë§ëŠ” ëª¨ë“ˆ ë°˜í™˜
        
        Args:
            layer: transformer layer ê°ì²´
            layer_type: 'ffn_down', 'ffn_up', 'attn_q', 'attn_k', 'attn_v'
                       Noneì´ë©´ self.args.layer_type ì‚¬ìš© (ë‹¨ì¼ íƒ€ì… í˜¸í™˜ì„±)
            
        Returns:
            target_module: ì„ íƒëœ projection ëª¨ë“ˆ
        """
        if layer_type is None:
            # ë‹¨ì¼ layer_type í˜¸í™˜ì„±ì„ ìœ„í•´ argsì—ì„œ ì²« ë²ˆì§¸ íƒ€ì… ì‚¬ìš©
            if isinstance(self.args.layer_type, str):
                layer_type = self.args.layer_type.split(',')[0].strip()
            else:
                layer_type = self.args.layer_type[0]
        
        if layer_type == 'ffn_down':
            return layer.mlp.down_proj
        elif layer_type == 'ffn_up':
            return layer.mlp.up_proj
        elif layer_type == 'attn_q':
            return layer.self_attn.q_proj
        elif layer_type == 'attn_k':
            return layer.self_attn.k_proj
        elif layer_type == 'attn_v':
            return layer.self_attn.v_proj
        else:
            raise ValueError(f"Unknown layer type: {layer_type}")
    
    def _parse_target_layers(self, num_layers):
        """íƒ€ê²Ÿ ë ˆì´ì–´ íŒŒì‹± (Phase 1ê³¼ ë™ì¼)"""
        target = self.args.target_layers.strip()
        
        if target == 'all':
            return list(range(num_layers))
        elif target == 'early':
            return list(range(0, min(11, num_layers)))
        elif target == 'middle':
            return list(range(11, min(22, num_layers)))
        elif target == 'late':
            return list(range(22, num_layers))
        elif target == 'last':
            return [num_layers - 1]
        
        if '-' in target:
            try:
                start, end = target.split('-')
                start, end = int(start.strip()), int(end.strip())
                return list(range(start, min(end + 1, num_layers)))
            except ValueError:
                raise ValueError(f"Invalid range format: {target}")
        
        try:
            layer_idx = int(target)
            if 0 <= layer_idx < num_layers:
                return [layer_idx]
            else:
                raise ValueError(f"Invalid layer index: {layer_idx}")
        except ValueError:
            raise ValueError(f"Invalid target_layers format: {target}")
