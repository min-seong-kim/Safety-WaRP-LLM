"""
Phase 3: Incremental Learning with Masked Updates
ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ì„ ë³´í˜¸í•˜ë©´ì„œ ìœ í‹¸ë¦¬í‹° ê°œì„ 
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from collections import OrderedDict
import os
import json
from tqdm import tqdm
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

# ============================================================
# ğŸ“Š GSM8K íŒŒì¸íŠœë‹ ê²€ì¦ ìœ í‹¸ë¦¬í‹°
# ============================================================
class GSM8KValidationMetrics:
    """GSM8K íŒŒì¸íŠœë‹ ì§„í–‰ ìƒí™©ì„ ì¶”ì í•˜ëŠ” ë©”íŠ¸ë¦­ í´ë˜ìŠ¤"""
    
    @staticmethod
    def is_converging(losses: list, min_improvement_pct: float = 1.0) -> bool:
        """ì†ì‹¤ì´ ìˆ˜ë ´í•˜ëŠ”ì§€ í™•ì¸"""
        if len(losses) < 2:
            return False
        initial = losses[0]
        final = losses[-1]
        improvement = (initial - final) / initial * 100
        return improvement > min_improvement_pct
    
    @staticmethod
    def is_stable(losses: list, max_cv: float = 20.0) -> bool:
        """ì†ì‹¤ì˜ ë³€ë™ì„±ì´ ì•ˆì •ì ì¸ì§€ í™•ì¸ (ë³€ë™ê³„ìˆ˜ ê¸°ë°˜)"""
        if len(losses) < 2:
            return True
        cv = np.std(losses) / np.mean(losses) * 100
        return cv < max_cv
    
    @staticmethod
    def has_gradient_flow(frozen_grad: float, trainable_grad: float) -> bool:
        """ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì •ìƒì ìœ¼ë¡œ íë¥´ëŠ”ì§€ í™•ì¸"""
        # Frozen ë°©í–¥ì˜ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ~0 ì´ê³ , Trainable ë°©í–¥ì€ > 0 ì´ì–´ì•¼ í•¨
        return frozen_grad < 1e-5 and trainable_grad > 1e-6


# ============================================================
# âœ… BasisLinear: torch.autograd.Functionìœ¼ë¡œ gradient flow êµ¬í˜„
# ============================================================
class BasisLinear(torch.autograd.Function):
    """
    Weightë¥¼ basis_coeff @ U^Të¡œ ë™ì  ì¬êµ¬ì„±í•˜ë©´ì„œ gradientë¥¼ ì •í™•í•˜ê²Œ ê³„ì‚°.
    
    Forward: y = (basis_coeff @ U^T) @ x^T + bias
    Backward: ë§ˆìŠ¤í‚¹ì„ ì ìš©í•˜ì—¬ frozen directionsì˜ gradient = 0
    
    Args:
        x: input (batch_size, in_features)
        basis_coeff: learnable parameters (out_features, rank)
        U: fixed basis matrix (in_features, rank)
        bias: bias (out_features)
        mask: binary mask (out_features,) - 1:frozen, 0:trainable
    """
    
    @staticmethod
    def forward(ctx, x, basis_coeff, U, bias, mask):
        """
        Forward pass: y = linear(x, weight, bias)
        where weight = basis_coeff @ U^T
        """
        # 1. Weight ì¬êµ¬ì„± (autogradê°€ ì¶”ì  ê°€ëŠ¥í•œ ì—°ì‚°)
        weight = basis_coeff @ U.T  # (out_features, in_features)
        
        # 2. Linear forward
        output = torch.nn.functional.linear(x, weight, bias)
        
        # 3. Backwardë¥¼ ìœ„í•´ í•„ìš”í•œ ì •ë³´ ì €ì¥
        # âœ… maskëŠ” í…ì„œê°€ ì•„ë‹ˆë¯€ë¡œ ctx.save_for_backwardì— í¬í•¨í•˜ë©´ ì•ˆ ë¨
        ctx.save_for_backward(x, basis_coeff, U, bias, weight)
        ctx.mask = mask  # maskëŠ” non-tensorë¡œ ì €ì¥
        
        # Debug: mask íƒ€ì… í™•ì¸
        # print(f"[BasisLinear.forward] mask type: {type(mask)}, requires_grad: {getattr(mask, 'requires_grad', 'N/A')}")
        
        return output
    
    @staticmethod
    def backward(ctx, grad_output):
        """
        Backward pass: ë§ˆìŠ¤í‚¹ì„ ì ìš©í•˜ì—¬ frozen directionsì˜ gradient ì œê±°
        
        grad_output: gradient w.r.t. output 
            - LLM ì¢…ë¥˜ë§ˆë‹¤ ë°°ì¹˜ ì°¨ì›ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
            - 2D: (batch_size, out_features)
            - 3D: (batch_size, seq_len, out_features) for LLM
        Returns: (grad_x, grad_basis_coeff, grad_U, grad_bias, grad_mask)
        """
        x, basis_coeff, U, bias, weight = ctx.saved_tensors
        mask = ctx.mask  # ê³ ì •ëœ mask ë³µì›
        
        # 3D ë°°ì¹˜ ì²˜ë¦¬ (LLMì˜ ê²½ìš° ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ í¬í•¨ë¨)
        original_shape = grad_output.shape
        if grad_output.dim() == 3:
            # (batch, seq_len, out_features) â†’ (batch*seq_len, out_features)
            batch_size, seq_len, out_features = grad_output.shape
            grad_output = grad_output.reshape(-1, out_features)
            x = x.reshape(-1, x.shape[-1])  # xë„ 3Dì´ë¯€ë¡œ reshape
        elif grad_output.dim() != 2:
            raise RuntimeError(f"grad_output must be 2D or 3D, got {original_shape}")
        
        # 1. Linear backward: dL/dW
        # y = x @ weight^T + bias
        # grad_output: (batch_size, out_features)
        # x: (batch_size, in_features)
        # grad_weight = grad_output.T @ x  (out_features, in_features)
        grad_weight = grad_output.T @ x
        
        # 2. Chain rule: dL/d(basis_coeff)
        # W = basis_coeff @ U^T
        # So: grad_basis_coeff = grad_weight @ U
        grad_basis_coeff = grad_weight @ U  # (out_features, rank) = (4096, 14336)
        
        # âœ… ë§ˆìŠ¤í‚¹ ì ìš©: frozen directions (mask=1) â†’ gradient = 0
        # mask shape: (14336,) - ê° input dimensionë³„ ë§ˆìŠ¤í‚¹
        # grad_basis_coeff shape: (4096, 14336) [out_features, rank/input_dim]
        # ë§ˆìŠ¤í‚¹: ê° input dimension (rank)ì— ëŒ€í•´ ëª¨ë“  outputì— ë™ì¼í•˜ê²Œ ì ìš©
        mask_expanded = (1 - mask).unsqueeze(0).detach()  # (1, 14336) - trainable ë°©í–¥ë§Œ 1, detach í•„ìˆ˜
        grad_basis_coeff = grad_basis_coeff * mask_expanded  # (4096, 14336) * (1, 14336) âœ“
        
        # 3. Gradient w.r.t. U
        # âš ï¸ UëŠ” fixed basisì´ë¯€ë¡œ requires_grad=False
        # ë”°ë¼ì„œ Noneì„ ë°˜í™˜
        grad_U = None
        
        # 4. Gradient w.r.t. input
        grad_x = grad_output @ weight  # (batch_size, in_features)
        
        # ë³µì›: 3D ì…ë ¥ì´ì—ˆìœ¼ë©´ ì›ë˜ shapeë¡œ ëŒë¦¼
        if len(original_shape) == 3:
            grad_x = grad_x.reshape(batch_size, seq_len, -1)
        
        # 5. Gradient w.r.t. bias
        grad_bias = grad_output.sum(dim=0)  # (out_features,)
        
        # âœ… CRITICAL: backwardëŠ” forwardì˜ inputsì™€ ê°™ì€ ê°œìˆ˜ì˜ gradientë¥¼ ë°˜í™˜í•´ì•¼ í•¨
        # forward inputs: (x, basis_coeff, U, bias, mask)
        # Uì™€ maskëŠ” ëª¨ë‘ non-trainableì´ë¯€ë¡œ Noneì„ ë°˜í™˜
        
        return grad_x, grad_basis_coeff, grad_U, grad_bias, None


class Phase3IncrementalLearner:
    """
    Phase 3: Incremental Learning with Masked Gradient Updates
    
    ì ˆì°¨:
    1. Phase 1 basis + Phase 2 masks ë¡œë“œ
    2. ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ basis ê³µê°„ìœ¼ë¡œ ì¬ë§¤ê°œë³€ìˆ˜í™”
    3. GSM8K ë°ì´í„°ë¡œ ë¯¸ì„¸ì¡°ì • (ë§ˆìŠ¤í‚¹ëœ gradient)
    4. ë§ˆìŠ¤í¬ëœ ë°©í–¥ (ì•ˆì „ ì¤‘ìš”)ì€ ì—…ë°ì´íŠ¸ ê¸ˆì§€
    5. ëœ ì¤‘ìš”í•œ ë°©í–¥ë§Œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
    """
    
    def __init__(self, args, logger, basis_dir, masks_dir):
        """
        Args:
            args: ì»¤ë§¨ë“œë¼ì¸ ì¸ì
            logger: ë¡œê±° ê°ì²´
            basis_dir: Phase 1 basis ë””ë ‰í† ë¦¬
            masks_dir: Phase 2 masks ë””ë ‰í† ë¦¬
        """
        self.args = args
        self.logger = logger
        self.basis_dir = basis_dir
        self.masks_dir = masks_dir
        
        # ëª¨ë¸ ë° ë°ì´í„°
        self.model = None
        self.tokenizer = None
        self.train_loader = None
        
        # Basis ì •ë³´
        self.basis_data = {}  # layer_idx -> {'U': U, 'S': S, 'Vh': Vh}
        
        # ë§ˆìŠ¤í¬
        self.masks = {}  # layer_idx -> binary mask
        
        # í›ˆë ¨ í†µê³„
        self.stats = {
            'epoch': 0,
            'total_loss': 0.0,
            'num_batches': 0,
        }
        
        # Hook ì €ì¥ìš©
        self.hook_handles = []
    
    def load_basis(self):
        """Phase 1ì—ì„œ ì €ì¥ëœ basis ë¡œë“œ"""
        try:
            self.logger.info(f"Loading basis from {self.basis_dir}...")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = os.path.join(self.basis_dir, 'metadata.json')
            with open(metadata_path, 'r') as f:
                basis_metadata = json.load(f)
            
            self.logger.info(f"âœ“ Metadata loaded:")
            self.logger.info(f"  - Target layers: {basis_metadata.get('target_layers')}")
            
            # basis íŒŒì¼ ë¡œë“œ
            import glob
            svd_files = sorted(glob.glob(os.path.join(self.basis_dir, 'layer_*_svd.pt')))
            
            for svd_path in svd_files:
                filename = os.path.basename(svd_path)
                layer_idx = int(filename.split('_')[1])
                
                svd_data = torch.load(svd_path, map_location='cpu')
                self.basis_data[layer_idx] = {
                    'U': svd_data['U'].to(self.args.device),
                    'S': svd_data['S'].to(self.args.device),
                    'Vh': svd_data['Vh'].to(self.args.device),
                }
            
            self.logger.info(f"âœ“ Basis loaded: {len(self.basis_data)} layers")
            self.logger.info(f"  - Layer indices: {sorted(self.basis_data.keys())}")
            
        except Exception as e:
            self.logger.error(f"Failed to load basis: {str(e)}", exc_info=True)
            raise
    
    def load_masks(self):
        """Phase 2ì—ì„œ ì €ì¥ëœ ë§ˆìŠ¤í¬ ë¡œë“œ"""
        try:
            self.logger.info(f"Loading masks from {self.masks_dir}...")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = os.path.join(self.masks_dir, 'metadata.json')
            with open(metadata_path, 'r') as f:
                masks_metadata = json.load(f)
            
            self.logger.info(f"âœ“ Mask metadata loaded:")
            self.logger.info(f"  - Keep ratio: {masks_metadata.get('keep_ratio')}")
            
            # ë§ˆìŠ¤í¬ íŒŒì¼ ë¡œë“œ
            import glob
            mask_files = sorted(glob.glob(os.path.join(self.masks_dir, 'layer_*_mask.pt')))
            
            for mask_path in mask_files:
                filename = os.path.basename(mask_path)
                layer_idx = int(filename.split('_')[1])
                
                mask = torch.load(mask_path, map_location='cpu')
                self.masks[layer_idx] = mask.to(self.args.device)
            
            self.logger.info(f"âœ“ Masks loaded: {len(self.masks)} layers")
            self.logger.info(f"  - Layer indices: {sorted(self.masks.keys())}")
            
            # ë§ˆìŠ¤í¬ í†µê³„
            for layer_idx in sorted(self.masks.keys()):
                mask = self.masks[layer_idx]
                num_important = (mask == 1).sum().item()
                ratio = num_important / mask.numel()
                self.logger.info(f"  - Layer {layer_idx}: {num_important}/{mask.numel()} important ({ratio*100:.2f}%)")
            
        except Exception as e:
            self.logger.error(f"Failed to load masks: {str(e)}", exc_info=True)
            raise
    
    def load_model(self):
        """
        â­ Phase 2ì˜ Safety Fine-tuned ëª¨ë¸ ë¡œë“œ
        
        ìš°ì„ ìˆœìœ„:
        1. Phase 2 safety fine-tuned ëª¨ë¸ì´ ìˆìœ¼ë©´ â†’ ê·¸ê²ƒ ë¡œë“œ (ê¶Œì¥)
        2. ì—†ìœ¼ë©´ â†’ ì›ë³¸ ëª¨ë¸ ë¡œë“œ (fallback)
        """
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        try:
            # ë°ì´í„° íƒ€ì… ì„¤ì •
            dtype_map = {
                'float32': torch.float32,
                'float16': torch.float16,
                'bfloat16': torch.bfloat16
            }
            torch_dtype = dtype_map.get(self.args.dtype, torch.bfloat16)
            
            # âœ… Step 1: Phase 2 safety fine-tuned ëª¨ë¸ ê²½ë¡œ í™•ì¸
            # êµ¬ì¡° ë¶„ì„:
            # masks_dir:          ./checkpoints/phase2_XXXX/checkpoints/masks/
            # phase2_finetuned:   ./checkpoints/phase2_XXXX/checkpoints/phase2_finetuned_model/
            #
            # masks_dirì˜ dirname â†’ ./checkpoints/phase2_XXXX/checkpoints/
            # ê±°ê¸°ì— phase2_finetuned_model ì¶”ê°€
            
            # masks_dir ì •ê·œí™” (í›„í–‰ slash ì œê±°)
            masks_dir_normalized = self.masks_dir.rstrip('/')
            
            # checkpoints ë””ë ‰í† ë¦¬ ê²½ë¡œ (masksì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)
            checkpoints_dir = os.path.dirname(masks_dir_normalized)  # ./checkpoints/phase2_XXXX/checkpoints
            
            phase2_model_dir = os.path.join(checkpoints_dir, 'phase2_finetuned_model')
            
            self.logger.debug(f"Searching Phase 2 model at: {phase2_model_dir}")
            
            model_to_load = None
            model_source = None
            
            if os.path.exists(phase2_model_dir) and os.path.isdir(phase2_model_dir):
                # Phase 2 fine-tuned ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
                # pytorch_model.bin (single file) ë˜ëŠ” safetensors í˜•ì‹ (distributed) ì²´í¬
                pytorch_bin_path = os.path.join(phase2_model_dir, 'pytorch_model.bin')
                safetensors_index_path = os.path.join(phase2_model_dir, 'model.safetensors.index.json')
                safetensors_single_path = os.path.join(phase2_model_dir, 'model.safetensors')
                
                has_pytorch_bin = os.path.exists(pytorch_bin_path)
                has_safetensors = os.path.exists(safetensors_index_path) or os.path.exists(safetensors_single_path)
                
                if has_pytorch_bin or has_safetensors:
                    model_to_load = phase2_model_dir
                    model_source = "Phase 2 Safety Fine-tuned"
                    model_format = "safetensors (distributed)" if os.path.exists(safetensors_index_path) else \
                                  "safetensors (single)" if os.path.exists(safetensors_single_path) else "pytorch_model.bin"
                    self.logger.debug(f"Found Phase 2 model at: {phase2_model_dir} (format: {model_format})")
            
            # âœ… Step 2: ëª¨ë¸ ë¡œë“œ
            if model_to_load is not None:
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"Loading {model_source} Model")
                self.logger.info(f"{'='*70}")
                self.logger.info(f"Model path: {model_to_load}")
                self.logger.info(f"Source: Phase 2 safety fine-tuning (do-not-answer dataset)")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_to_load,
                    torch_dtype=torch_dtype,
                    device_map=self.args.device,
                    trust_remote_code=True
                )
                self.logger.info(f"âœ“ {model_source} model loaded successfully!")
                self.logger.info(f"  This model has been fine-tuned on safety data (do-not-answer)")
                self.logger.info(f"  and should refuse harmful requests better than the base model.")
                
                # í† í¬ë‚˜ì´ì €ë„ Phase 2 ëª¨ë¸ì—ì„œ ë¡œë“œ
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_to_load,
                    trust_remote_code=True
                )
            else:
                # Fallback: ì›ë³¸ ëª¨ë¸ ë¡œë“œ (Phase 2 ëª¨ë¸ì´ ì—†ìœ¼ë©´)
                self.logger.warning(f"\n{'='*70}")
                self.logger.warning(f"âš ï¸ Phase 2 Safety Fine-tuned Model Not Found!")
                self.logger.warning(f"{'='*70}")
                self.logger.warning(f"Expected path: {phase2_model_dir}")
                self.logger.warning(f"Falling back to original model: {self.args.model_name}")
                self.logger.warning(f"âš ï¸ WARNING: Phase 3 will train on original (unsafe) model!")
                self.logger.warning(f"   This may result in a model that is not safety-aligned.")
                self.logger.warning(f"{'='*70}\n")
                
                self.logger.info(f"Loading original model: {self.args.model_name}")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.args.model_name,
                    torch_dtype=torch_dtype,
                    device_map=self.args.device,
                    trust_remote_code=True
                )
                
                self.logger.info(f"âœ“ Original model loaded (fallback)")
                
                # í† í¬ë‚˜ì´ì €ë„ ì›ë³¸ì—ì„œ ë¡œë“œ
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.args.model_name,
                    trust_remote_code=True
                )
            
            # âœ… Step 3: í† í¬ë‚˜ì´ì € ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.logger.info(f"âœ“ Tokenizer loaded successfully")
            self.logger.info(f"  - Vocab size: {self.tokenizer.vocab_size}")
            self.logger.info(f"  - Pad token: {self.tokenizer.pad_token}")
            self.logger.info(f"  - EOS token: {self.tokenizer.eos_token}\n")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}", exc_info=True)
            raise
    
    def load_utility_data(self):
        """GSM8K í›ˆë ¨ ë°ì´í„° ë¡œë“œ"""
        from datasets import load_dataset
        
        try:
            self.logger.info(f"Loading GSM8K (main/train) with max_samples={self.args.utility_samples}...")
            
            dataset = load_dataset('openai/gsm8k', 'main', split='train')
            
            # ìƒ˜í”Œ ìˆ˜ ì œí•œ
            if self.args.utility_samples > 0:
                dataset = dataset.select(range(min(self.args.utility_samples, len(dataset))))
            
            self.logger.info(f"âœ“ Dataset loaded: {len(dataset)} samples")
            
            # ë°ì´í„°ì…‹ ì¤€ë¹„ í•¨ìˆ˜
            def format_gsm8k(examples):
                # question -> input, answerëŠ” ì „ì²´ í’€ì´ ê³¼ì •
                formatted_inputs = []
                formatted_targets = []
                
                for question, answer in zip(examples['question'], examples['answer']):
                    formatted_inputs.append(f"Q: {question}\nA:")
                    formatted_targets.append(answer)
                
                return {
                    'input_text': formatted_inputs,
                    'target_text': formatted_targets,
                }
            
            dataset = dataset.map(format_gsm8k, batched=True, batch_size=100)
            
            # ë°ì´í„°ë¡œë” ìƒì„±
            class GSM8KDataset(torch.utils.data.Dataset):
                def __init__(self, dataset, tokenizer, max_length=512):
                    self.dataset = dataset
                    self.tokenizer = tokenizer
                    self.max_length = max_length
                
                def __len__(self):
                    return len(self.dataset)
                
                def __getitem__(self, idx):
                    sample = self.dataset[idx]
                    input_text = sample['input_text']
                    target_text = sample['target_text']
                    
                    # ê²°í•©: "Q: ...\nA: <answer>"
                    combined = f"{input_text}{target_text}"
                    
                    # í† í¬ë‚˜ì´ì œì´ì…˜
                    encoding = self.tokenizer(
                        combined,
                        truncation=True,
                        max_length=self.max_length,
                        return_tensors='pt'
                    )
                    
                    return {
                        'input_ids': encoding['input_ids'].squeeze(),
                        'attention_mask': encoding['attention_mask'].squeeze(),
                    }
            
            gsm8k_dataset = GSM8KDataset(dataset, self.tokenizer, max_length=512)
            
            # Custom collate function for variable length sequences
            def collate_fn_gsm8k(batch):
                """ê¸¸ì´ê°€ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ë¥¼ paddingìœ¼ë¡œ ì²˜ë¦¬"""
                max_len = max(len(item['input_ids']) for item in batch)
                
                input_ids_list = []
                attention_masks_list = []
                
                for item in batch:
                    input_ids = item['input_ids']
                    attn_mask = item['attention_mask']
                    
                    # Padding
                    pad_len = max_len - len(input_ids)
                    if pad_len > 0:
                        input_ids = torch.nn.functional.pad(
                            input_ids.unsqueeze(0),
                            (0, pad_len),
                            value=self.tokenizer.pad_token_id
                        ).squeeze(0)
                        attn_mask = torch.nn.functional.pad(
                            attn_mask.unsqueeze(0),
                            (0, pad_len),
                            value=0
                        ).squeeze(0)
                    
                    input_ids_list.append(input_ids)
                    attention_masks_list.append(attn_mask)
                
                return {
                    'input_ids': torch.stack(input_ids_list),
                    'attention_mask': torch.stack(attention_masks_list),
                }
            
            self.train_loader = DataLoader(
                gsm8k_dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                collate_fn=collate_fn_gsm8k
            )
            
            self.logger.info(f"âœ“ Utility dataloader created:")
            self.logger.info(f"  - Batch size: {self.args.batch_size}")
            self.logger.info(f"  - Total batches: {len(self.train_loader)}")
            
        except Exception as e:
            self.logger.error(f"Failed to load utility data: {str(e)}", exc_info=True)
            raise
    
    def register_mask_hooks(self):
        """
        âœ… Custom forward í•¨ìˆ˜ë¡œ BasisLinear.apply ì‚¬ìš©
        
        Hook ëŒ€ì‹  module.forwardë¥¼ êµì²´í•˜ì—¬ autograd.Functionìœ¼ë¡œ gradient ê³„ì‚°
        """
        # New robust implementation: do NOT use custom autograd.Function.
        # Instead register a Python-level custom forward that reconstructs weight
        # from a learnable `basis_coeff` (so autograd tracks it), and keep the
        # per-input (element-wise) mask to zero-out gradients after backward.
        try:
            self.original_forwards = {}
            self._basis_params = []  # list of parameters to optimize

            # Freeze all existing model parameters to avoid accidental updates
            for p in self.model.parameters():
                p.requires_grad = False

            for layer_idx in sorted(self.masks.keys()):
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj

                # load mask and U
                mask = self.masks[layer_idx]
                if isinstance(mask, np.ndarray):
                    mask = torch.from_numpy(mask)
                mask = mask.to(self.model.device).to(torch.bool)

                U = self.basis_data[layer_idx]['U'].to(self.model.device)
                W_original = target_module.weight.data.clone()
                U = U.to(dtype=W_original.dtype, device=W_original.device)

                # Create basis_coeff as a Parameter so autograd computes grads w.r.t it
                basis_coeff = (W_original @ U).clone().detach()
                basis_coeff = nn.Parameter(basis_coeff)
                # attach to module so we can access it later
                target_module.register_parameter('basis_coeff', basis_coeff)
                target_module.U_matrix = U
                target_module._warp_mask = mask  # boolean mask on input dims

                # Keep track for optimizer
                self._basis_params.append(target_module.basis_coeff)

                # Save original forward to restore later
                self.original_forwards[layer_idx] = target_module.forward

                # Custom forward: compute weight = basis_coeff @ U.T and run linear
                def make_custom_forward(basis_name='basis_coeff'):
                    def custom_forward(x):
                        module = getattr(custom_forward, '_module')
                        basis = getattr(module, basis_name)
                        U_local = module.U_matrix
                        weight = basis @ U_local.T
                        return torch.nn.functional.linear(x, weight, module.bias)
                    return custom_forward

                custom_fn = make_custom_forward('basis_coeff')
                setattr(custom_fn, '_module', target_module)
                target_module.forward = custom_fn

                # Logging
                self.logger.info(f"âœ“ Layer {layer_idx}: registered basis_coeff")
                self.logger.info(f"  - basis_coeff shape: {target_module.basis_coeff.shape}")
                self.logger.info(f"  - mask shape: {mask.shape}")
                self.logger.info(f"  - Frozen (True) count: {mask.sum().item()}/{mask.numel()} ({100*mask.sum().item()/mask.numel():.1f}%)")

            self.logger.info(f"âœ… {len(self.original_forwards)} layers configured for masked fine-tuning")

        except Exception as e:
            self.logger.error(f"Failed to register mask hooks: {str(e)}", exc_info=True)
            raise
    
    def restore_original_forwards(self):
        """Forward í•¨ìˆ˜ ë³µì›"""
        try:
            for layer_idx, original_forward in self.original_forwards.items():
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj
                target_module.forward = original_forward
                # remove attached parameter if exists
                if hasattr(target_module, 'basis_coeff'):
                    try:
                        delattr(target_module, 'basis_coeff')
                    except Exception:
                        pass
                if hasattr(target_module, '_warp_mask'):
                    try:
                        delattr(target_module, '_warp_mask')
                    except Exception:
                        pass
            self.logger.info(f"âœ“ {len(self.original_forwards)} original forwards restored")
        except Exception as e:
            self.logger.error(f"Failed to restore forwards: {str(e)}", exc_info=True)
    
    def _reconstruct_and_save_final_model(self):
        """
        âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì „: basis_coeff @ U^Të¥¼ ê³„ì‚°í•˜ì—¬ ì›ë³¸ weightë¡œ ë³µì›
        
        í•™ìŠµëœ basis_coeffë¥¼ ì‚¬ìš©í•´ ìµœì¢… ê°€ì¤‘ì¹˜ë¥¼ ì¬êµ¬ì„±í•˜ê³ ,
        ì´ë¥¼ ëª¨ë¸ì˜ weightë¡œ ì„¤ì •í•œ í›„ ì €ì¥.
        basis_coeff íŒŒë¼ë¯¸í„°ëŠ” state_dictì—ì„œ ì œê±°.
        """
        try:
            self.logger.info("  Reconstructing weights from learned basis_coeff...")
                                     
            for layer_idx in sorted(self.masks.keys()):
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj
                
                if not hasattr(target_module, 'basis_coeff'):
                    self.logger.warning(f"  Layer {layer_idx}: basis_coeff not found, skipping")
                    continue
                
                # basis_coeffì™€ U ë¡œë“œ
                basis_coeff = target_module.basis_coeff.data  # (out_features, rank)
                U = target_module.U_matrix  # (in_features, rank)
                
                # ìµœì¢… ê°€ì¤‘ì¹˜ ì¬êµ¬ì„±: W = basis_coeff @ U^T
                final_weight = basis_coeff @ U.T  # (out_features, in_features)
                
                # ëª¨ë¸ì˜ weight ì—…ë°ì´íŠ¸
                target_module.weight.data = final_weight
                
                self.logger.info(f"    âœ“ Layer {layer_idx}: weight reconstructed (shape: {final_weight.shape})")
            
            # âœ… ìµœì¢… ëª¨ë¸ ì €ì¥
            checkpoint_dir = os.path.join(self.args.checkpoint_dir, 'checkpoints')
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # âœ… state_dict ë¡œë“œ í›„ basis_coeff íŒŒë¼ë¯¸í„° ì œê±°
            model_state_dict = self.model.state_dict()
            
            # basis_coeff í‚¤ ì œê±°
            basis_coeff_keys = [k for k in model_state_dict.keys() if 'basis_coeff' in k]
            for key in basis_coeff_keys:
                del model_state_dict[key]
                self.logger.info(f"    âœ“ Removed {key} from state_dict")
            
            final_checkpoint = {
                'epoch': self.args.epochs - 1,
                'model_state_dict': model_state_dict,
                'config': vars(self.args),
                'metadata': {
                    'phase': 'phase3',
                    'basis_reconstruction': True,
                    'description': 'Final model with reconstructed weights from basis_coeff @ U^T'
                }
            }
            
            final_path = os.path.join(checkpoint_dir, 'phase3_final_reconstructed.pt')
            torch.save(final_checkpoint, final_path)
            self.logger.info(f"  âœ“ Final reconstructed model saved: {final_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to reconstruct and save final model: {str(e)}", exc_info=True)
            raise
    
    def train_epoch(self, epoch: int, optimizer, lr_scheduler=None):
        """
        í•œ ì—í¬í¬ í›ˆë ¨ (ìƒì„¸ ë¡œê¹… í¬í•¨)
        
        Args:
            epoch: ì—í¬í¬ ë²ˆí˜¸
            optimizer: ì˜µí‹°ë§ˆì´ì €
            lr_scheduler: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        """
        self.model.train()
        
        total_loss = 0.0
        total_frozen_grad_norm = 0.0
        total_trainable_grad_norm = 0.0
        total_masked_grad_norm = 0.0  # ë§ˆìŠ¤í‚¹ ì „ gradient norm
        total_param_norm = 0.0  # íŒŒë¼ë¯¸í„° norm
        total_tokens = 0  # ì´ í† í° ìˆ˜
        num_batches = 0
        
        # ë°°ì¹˜ë³„ ìƒì„¸ ë¡œê¹…ì„ ìœ„í•œ ì €ì¥ì†Œ
        batch_logs = []
        
        # GSM8K íŒŒì¸íŠœë‹ ê²€ì¦ìš© ë³€ìˆ˜
        loss_improvements = []  # ì†ì‹¤ ê°œì„  ì¶”ì´ ì¶”ì 
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {epoch+1}/{self.args.epochs}",
            total=len(self.train_loader)
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            input_ids = batch['input_ids'].to(self.model.device)
            attention_mask = batch['attention_mask'].to(self.model.device)
            
            # ë°°ì¹˜ í†µê³„
            batch_size = input_ids.shape[0]
            seq_length = input_ids.shape[1]
            num_tokens_batch = (input_ids != self.tokenizer.pad_token_id).sum().item()
            total_tokens += num_tokens_batch
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
            
            # Forward pass: CLMìœ¼ë¡œ í•™ìŠµ
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids
            )
            loss = outputs.loss
            
            # âœ… Gradient accumulation: lossë¥¼ accumulation_stepsë¡œ ë‚˜ëˆ„ê¸°
            scaled_loss = loss / self.gradient_accumulation_steps
            
            # âœ… GSM8K íŒŒì¸íŠœë‹ ê²€ì¦: ì†ì‹¤ ê°’ ê¸°ë¡ (ìŠ¤ì¼€ì¼ë§ë˜ì§€ ì•Šì€ ì›ë³¸ loss ê¸°ë¡)
            loss_improvements.append(loss.item())
            
            # Backward pass
            scaled_loss.backward()
            
            # âœ… Gradient accumulation ìŠ¤í…ì¸ì§€ í™•ì¸
            should_step = ((batch_idx + 1) % self.gradient_accumulation_steps == 0) or (batch_idx == len(self.train_loader) - 1)
            
            # After backward, apply element-wise mask to basis_coeff.grad to freeze important inputs
            batch_frozen_grad = 0.0
            batch_trainable_grad = 0.0
            batch_masked_before = 0.0
            batch_param_norm = 0.0
            layer_logs = []

            for layer_idx in sorted(self.masks.keys()):
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj

                if not hasattr(target_module, 'basis_coeff'):
                    continue

                basis_param = target_module.basis_coeff
                if basis_param.grad is None:
                    continue

                # mask: boolean on input dimension (d_in,)
                mask = target_module._warp_mask
                frozen_idx = mask
                trainable_idx = ~mask

                # basis_param.grad shape: (d_out, d_in)
                pre_norm = basis_param.grad.norm().item() if basis_param.grad.numel() > 0 else 0.0
                batch_masked_before += pre_norm

                # ë§ˆìŠ¤í‚¹ ì „ frozen/trainable ë¶„ì„
                if frozen_idx.any():
                    frozen_grad_before = basis_param.grad[:, frozen_idx]
                    frozen_norm_before = torch.norm(frozen_grad_before).item() if frozen_grad_before.numel() > 0 else 0.0
                else:
                    frozen_norm_before = 0.0

                # âœ… ë§ˆìŠ¤í‚¹ ì ìš©: WaRPì˜ í•µì‹¬ - frozen directionì˜ gradientë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                if frozen_idx.any():
                    basis_param.grad[:, frozen_idx] = 0.0

                post_norm = basis_param.grad.norm().item() if basis_param.grad.numel() > 0 else 0.0

                # statistics for logging
                frozen_grad_norm = 0.0
                trainable_grad_norm = 0.0
                if frozen_idx.any():
                    frozen_grad = basis_param.grad[:, frozen_idx]
                    frozen_grad_norm = torch.norm(frozen_grad).item() if frozen_grad.numel() > 0 else 0.0
                if trainable_idx.any():
                    trainable_grad = basis_param.grad[:, trainable_idx]
                    trainable_grad_norm = torch.norm(trainable_grad).item() if trainable_grad.numel() > 0 else 0.0

                # íŒŒë¼ë¯¸í„° norm
                param_norm = basis_param.norm().item()

                batch_frozen_grad += frozen_grad_norm
                batch_trainable_grad += trainable_grad_norm
                batch_param_norm += param_norm
                total_frozen_grad_norm += frozen_grad_norm
                total_trainable_grad_norm += trainable_grad_norm
                total_param_norm += param_norm

                # ë ˆì´ì–´ë³„ ë¡œê·¸ ì €ì¥
                layer_logs.append({
                    'layer_idx': layer_idx,
                    'grad_pre_mask': pre_norm,
                    'grad_post_mask': post_norm,
                    'frozen_grad_before': frozen_norm_before,
                    'frozen_grad_after': frozen_grad_norm,
                    'trainable_grad': trainable_grad_norm,
                    'param_norm': param_norm,
                    'num_frozen': frozen_idx.sum().item(),
                    'num_trainable': trainable_idx.sum().item(),
                })
            
            # âœ… Gradient accumulation: ìŠ¤í…ì—ì„œë§Œ ì—…ë°ì´íŠ¸
            if should_step:
                # âœ… Gradient clipping (max_grad_norm=0.3, finetune_gsm8k.pyì™€ ë™ì¼)
                total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.3)
                
                # Update
                optimizer.step()
                optimizer.zero_grad()
                
                if lr_scheduler:
                    lr_scheduler.step()
                
                num_batches += 1
            
            total_loss += loss.item()  # ìŠ¤ì¼€ì¼ë§ë˜ì§€ ì•Šì€ ì›ë³¸ loss ëˆ„ì 
            total_masked_grad_norm += batch_masked_before
            
            # ë°°ì¹˜ ë¡œê·¸ ì €ì¥
            batch_logs.append({
                'batch_idx': batch_idx,
                'loss': loss.item(),
                'seq_length': input_ids.shape[1],
                'batch_size': input_ids.shape[0],
                'num_tokens': num_tokens_batch,
                'frozen_grad': batch_frozen_grad,
                'trainable_grad': batch_trainable_grad,
                'param_norm': batch_param_norm,
                'layer_logs': layer_logs,
                'is_update_step': should_step,
            })
            
            # âœ… GSM8K íŒŒì¸íŠœë‹ ê²€ì¦: ì†ì‹¤ ê°œì„ ë„ ê³„ì‚°
            if len(loss_improvements) > 1:
                loss_delta = loss_improvements[-2] - loss_improvements[-1]
            else:
                loss_delta = 0.0
            
            # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸
            progress_bar.update(1)
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'frzn_norm': f'{batch_frozen_grad:.6f}',
                'train_norm': f'{batch_trainable_grad:.4f}',
                'accum': f'{((batch_idx + 1) % self.gradient_accumulation_steps) or self.gradient_accumulation_steps}/{self.gradient_accumulation_steps}' if not should_step else 'âœ“'
            })
            
            # ë§¤ Nê°œ ë°°ì¹˜ë§ˆë‹¤ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
            log_interval = max(1, len(self.train_loader) // 10)  # ì—í¬í¬ë‹¹ 10ë²ˆ ì¶œë ¥
            if (batch_idx + 1) % log_interval == 0:
                current_lr = optimizer.param_groups[0]['lr']
                
                # âœ… GSM8K íŒŒì¸íŠœë‹ ê²€ì¦ ì •ë³´
                avg_loss_recent = np.mean(loss_improvements[-log_interval:]) if len(loss_improvements) >= log_interval else np.mean(loss_improvements)
                loss_trend = "â†“" if loss_delta < 0 else ("â†’" if abs(loss_delta) < 1e-5 else "â†‘")
                
                self.logger.info(
                    f"[Batch {batch_idx+1:4d}/{len(self.train_loader)}] "
                    f"Loss: {loss.item():.4f} (avg: {avg_loss_recent:.4f}) {loss_trend} | "
                    f"Frozen grad: {batch_frozen_grad:.6f} | "
                    f"Trainable grad: {batch_trainable_grad:.4f} | "
                    f"Param norm: {batch_param_norm:.4f} | "
                    f"Tokens: {num_tokens_batch}/{seq_length*batch_size} | "
                    f"LR: {current_lr:.2e} | "
                    f"Update steps: {num_batches}"
                )
                
                # ë ˆì´ì–´ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥
                for layer_log in layer_logs:
                    self.logger.debug(
                        f"  Layer {layer_log['layer_idx']}: "
                        f"grad_pre={layer_log['grad_pre_mask']:.6f} "
                        f"grad_post={layer_log['grad_post_mask']:.6f} "
                        f"frozen_before={layer_log['frozen_grad_before']:.6f} "
                        f"frozen_after={layer_log['frozen_grad_after']:.6f} | "
                        f"trainable={layer_log['trainable_grad']:.4f} | "
                        f"frozen({layer_log['num_frozen']}) "
                        f"trainable({layer_log['num_trainable']})"
                    )
        
        # ì—í¬í¬ í†µê³„ (num_batchesëŠ” gradient accumulation ìŠ¤í… íšŸìˆ˜)
        avg_loss = total_loss / max(len(batch_logs), 1)  # ëª¨ë“  forward passë¡œ ë‚˜ëˆ„ê¸°
        num_update_steps = num_batches
        
        avg_frozen_grad = total_frozen_grad_norm / max(len(batch_logs), 1)
        avg_trainable_grad = total_trainable_grad_norm / max(len(batch_logs), 1)
        avg_masked_before = total_masked_grad_norm / max(len(batch_logs), 1)
        avg_param_norm = total_param_norm / max(len(batch_logs), 1)
        
        # âœ… GSM8K íŒŒì¸íŠœë‹ ê²€ì¦: ì†ì‹¤ ìˆ˜ë ´ ë¶„ì„
        loss_array = np.array(loss_improvements)
        loss_first_half = np.mean(loss_array[:len(loss_array)//2]) if len(loss_array) > 0 else float('inf')
        loss_second_half = np.mean(loss_array[len(loss_array)//2:]) if len(loss_array) > 0 else float('inf')
        loss_convergence = loss_first_half - loss_second_half
        loss_convergence_pct = (loss_convergence / loss_first_half * 100) if loss_first_half > 0 else 0
        
        # ì†ì‹¤ ë³€ë™ì„± ë¶„ì„
        loss_std = np.std(loss_array) if len(loss_array) > 0 else 0
        loss_cv = (loss_std / avg_loss * 100) if avg_loss > 0 else 0  # ë³€ë™ê³„ìˆ˜
        
        # ì´ˆë°˜ vs í›„ë°˜ ì†ì‹¤
        loss_min = np.min(loss_array) if len(loss_array) > 0 else float('inf')
        loss_max = np.max(loss_array) if len(loss_array) > 0 else float('inf')
        loss_improvement_ratio = (loss_max - loss_min) / loss_max * 100 if loss_max > 0 else 0
        
        # ì—í¬í¬ ì™„ë£Œ ë¡œê·¸
        self.logger.info(f"\n{'='*70}")
        self.logger.info(f"Epoch {epoch+1} Summary - GSM8K íŒŒì¸íŠœë‹ (SFT ìŠ¤íƒ€ì¼ í›ˆë ¨)")
        self.logger.info(f"{'='*70}")
        self.logger.info(f"  [ì†ì‹¤ í•¨ìˆ˜]")
        self.logger.info(f"    â€¢ í‰ê·  ì†ì‹¤ (Loss): {avg_loss:.4f}")
        self.logger.info(f"    â€¢ ìµœì € ì†ì‹¤: {loss_min:.4f}")
        self.logger.info(f"    â€¢ ìµœê³  ì†ì‹¤: {loss_max:.4f}")
        self.logger.info(f"    â€¢ ì†ì‹¤ ê°œì„ ìœ¨: {loss_improvement_ratio:.2f}% (ë²”ìœ„ {loss_max:.4f} â†’ {loss_min:.4f})")
        self.logger.info(f"    â€¢ ì†ì‹¤ í‘œì¤€í¸ì°¨: {loss_std:.6f}")
        self.logger.info(f"    â€¢ ì†ì‹¤ ë³€ë™ê³„ìˆ˜ (CV): {loss_cv:.2f}% {'âœ“' if loss_cv < 15 else 'âš ï¸'} (CV < 15% ê¶Œì¥)")
        self.logger.info(f"    â€¢ ì „ë°˜ë¶€ vs í›„ë°˜ë¶€ ìˆ˜ë ´: {loss_convergence:.4f} ({loss_convergence_pct:.2f}%) {'âœ“' if loss_convergence > 0 else 'âš ï¸'}")
        
        self.logger.info(f"  [ê·¸ë˜ë””ì–¸íŠ¸ íë¦„]")
        self.logger.info(f"    â€¢ Frozen ë°©í–¥ ê·¸ë˜ë””ì–¸íŠ¸: {avg_frozen_grad:.6f} (expected ~0) {'âœ“' if avg_frozen_grad < 1e-5 else 'âš ï¸'}")
        self.logger.info(f"    â€¢ Trainable ë°©í–¥ ê·¸ë˜ë””ì–¸íŠ¸: {avg_trainable_grad:.4f}")
        self.logger.info(f"    â€¢ ê·¸ë˜ë””ì–¸íŠ¸ ë¹„ìœ¨ (Trainable/Frozen): {avg_trainable_grad/max(avg_frozen_grad, 1e-8):.1f}x")
        
        self.logger.info(f"  [ë°ì´í„° ë° í›ˆë ¨ í†µê³„]")
        self.logger.info(f"    â€¢ ì´ Forward íŒ¨ìŠ¤: {len(batch_logs)}")
        self.logger.info(f"    â€¢ ì´ Gradient Accumulation ìŠ¤í… (ì—…ë°ì´íŠ¸): {num_update_steps}")
        self.logger.info(f"    â€¢ Accumulation ë¹„ìœ¨: {self.gradient_accumulation_steps}x")
        self.logger.info(f"    â€¢ ì´ í† í°: {total_tokens:,}")
        self.logger.info(f"    â€¢ ë°°ì¹˜ë‹¹ í‰ê·  í† í°: {total_tokens / max(len(batch_logs), 1):.1f}")
        self.logger.info(f"    â€¢ ë°°ì¹˜ë‹¹ í‰ê·  ì‹œí€€ìŠ¤ ê¸¸ì´: {sum(b['seq_length'] for b in batch_logs) / max(len(batch_logs), 1):.1f}")
        
        self.logger.info(f"  [íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸]")
        self.logger.info(f"    â€¢ íŒŒë¼ë¯¸í„° norm: {avg_param_norm:.4f}")
        
        self.logger.info(f"  [í•™ìŠµ ê±´ê°•ë„]")
        # íŒì • ì¡°ê±´: (1) ê·¸ë˜ë””ì–¸íŠ¸ ì •ìƒ íë¦„, (2) ì†ì‹¤ì´ ê°ì†Œí•˜ê±°ë‚˜ ì•ˆì •ì , (3) Trainable ê·¸ë˜ë””ì–¸íŠ¸ ì¡´ì¬
        is_gradient_ok = avg_frozen_grad < 1e-5 and avg_trainable_grad > 1e-6
        is_loss_stable = loss_cv < 25  # CV < 25%ë¡œ ì™„í™”
        is_converging = loss_convergence_pct > 0.3  # ìˆ˜ë ´ ê¸°ì¤€ ì™„í™” (0.3% ì´ìƒ)
        
        if is_gradient_ok and is_loss_stable:
            if loss_convergence_pct > 1.0:
                self.logger.info(f"    âœ… íŒŒì¸íŠœë‹ ì§„í–‰ ì¤‘! (ì†ì‹¤ ìˆ˜ë ´ O, ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ O, SFT ìŠ¤íƒ€ì¼ í›ˆë ¨ ì ìš©ë¨)")
            elif is_converging:
                self.logger.info(f"    âœ… íŒŒì¸íŠœë‹ ì§„í–‰ ì¤‘! (ì†ì‹¤ ì•ˆì •ì , ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ O, SFT ìŠ¤íƒ€ì¼ í›ˆë ¨ ì ìš©ë¨)")
            else:
                self.logger.info(f"    âš ï¸ íŒŒì¸íŠœë‹ ì§„í–‰ (ì†ì‹¤ì´ ì²œì²œíˆ ìˆ˜ë ´ ì¤‘)")
        else:
            if not is_gradient_ok:
                self.logger.info(f"    âš ï¸ ì£¼ì˜: ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë¬¸ì œ (Frozen: {avg_frozen_grad:.6f})")
            if not is_loss_stable:
                self.logger.info(f"    âš ï¸ ì£¼ì˜: ì†ì‹¤ ë³€ë™ì„± ë†’ìŒ (CV: {loss_cv:.2f}%)")
        self.logger.info(f"{'='*70}\n")
        
        return avg_loss
    
    def train(self):
        """
        ì „ì²´ í›ˆë ¨ ë£¨í”„ (ìƒì„¸ ë¡œê¹… í¬í•¨)
        """
        try:
            self.logger.info("\n" + "="*70)
            self.logger.info("PHASE 3: INCREMENTAL LEARNING WITH MASKED GRADIENT UPDATES")
            self.logger.info("="*70 + "\n")
            
            # 1. ë°ì´í„° ë° ëª¨ë¸ ë¡œë“œ
            self.logger.info("[Step 1] Loading basis and masks...")
            self.load_basis()
            self.load_masks()
            
            self.logger.info("\n[Step 2] Loading model...")
            self.load_model()
            
            self.logger.info("\n[Step 3] Loading utility data...")
            start_time = datetime.now()
            self.load_utility_data()
            load_time = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"âœ“ Data loading completed in {load_time:.2f}s")
            
            # 2. ë§ˆìŠ¤í‚¹ hook ë“±ë¡
            self.logger.info("\n[Step 4] Registering mask hooks...")
            self.register_mask_hooks()
            
            # ë§ˆìŠ¤í‚¹ ê²€ì¦
            self.logger.info("\n[Step 4.5] Validating mask configuration...")
            total_frozen = 0
            total_trainable = 0
            for layer_idx in sorted(self.masks.keys()):
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj
                mask = target_module._warp_mask
                num_frozen = (mask == 1).sum().item()
                num_trainable = (mask == 0).sum().item()
                total_frozen += num_frozen
                total_trainable += num_trainable
                self.logger.info(
                    f"  Layer {layer_idx}: {num_frozen}/{mask.numel()} frozen "
                    f"({100*num_frozen/mask.numel():.1f}%) | "
                    f"{num_trainable} trainable"
                )
            self.logger.info(f"âœ“ Total dimensions: {total_frozen} frozen, {total_trainable} trainable")
            
            # 3. Optimizer ì„¤ì •
            self.logger.info("\n[Step 5] Setting up optimizer and scheduler (SFTTrainer style)...")
            
            # âœ… gradient accumulation ì„¤ì • (effective batch: 4 * 16 = 64)
            self.gradient_accumulation_steps = 16
            
            # âœ… gradient checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆê°: ~50%)
            if hasattr(self.model.config, 'gradient_checkpointing'):
                self.model.gradient_checkpointing_enable()
                self.logger.info(f"âœ“ Gradient checkpointing enabled")
            
            optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
            
            # âœ… Learning rate scheduler: Cosine annealing with warmup (SFTTrainer ë°©ì‹)
            total_steps = len(self.train_loader) * self.args.epochs
            warmup_steps = int(total_steps * 0.05)  # 5% warmup (finetune_gsm8k.pyì™€ ë™ì¼)
            
            def cosine_lr_lambda(step):
                """Cosine annealing with linear warmup"""
                if step < warmup_steps:
                    return float(step) / float(max(1, warmup_steps))
                progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))
                return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))
            
            lr_scheduler = optim.lr_scheduler.LambdaLR(
                optimizer,
                lr_lambda=cosine_lr_lambda
            )
            
            self.logger.info(f"âœ“ Optimizer configured (matching SFTTrainer):")
            self.logger.info(f"  - Algorithm: AdamW")
            self.logger.info(f"  - Learning rate (initial): {self.args.learning_rate}")
            self.logger.info(f"  - Weight decay: {self.args.weight_decay}")
            self.logger.info(f"  - Batch size: {self.args.batch_size}")
            self.logger.info(f"  - Gradient accumulation steps: {self.gradient_accumulation_steps}")
            self.logger.info(f"  - Effective batch size: {self.args.batch_size * self.gradient_accumulation_steps}")
            self.logger.info(f"  - Scheduler: Cosine decay with 5% warmup")
            self.logger.info(f"  - Total training steps: {total_steps}")
            self.logger.info(f"  - Warmup steps: {warmup_steps}")
            self.logger.info(f"  - Max grad norm: 0.3")
            self.logger.info(f"  - Gradient checkpointing: Enabled")
            
            # 4. í›ˆë ¨
            self.logger.info("\n[Step 6] Starting training...")
            self.logger.info(f"  - Total epochs: {self.args.epochs}")
            self.logger.info(f"  - Batches per epoch: {len(self.train_loader)}")
            self.logger.info(f"  - Batch size: {self.args.batch_size}")
            self.logger.info(f"  - Total samples: {len(self.train_loader) * self.args.batch_size * self.args.epochs}")
            self.logger.info(f"\n{'='*70}\n")
            
            best_loss = float('inf')
            training_start = datetime.now()
            epoch_losses = []
            
            for epoch in range(self.args.epochs):
                epoch_start = datetime.now()
                
                self.logger.info(f"{'='*70}")
                self.logger.info(f"Epoch {epoch+1}/{self.args.epochs}")
                self.logger.info(f"{'='*70}")
                
                avg_loss = self.train_epoch(epoch, optimizer, lr_scheduler)
                
                epoch_time = (datetime.now() - epoch_start).total_seconds()
                current_lr = optimizer.param_groups[0]['lr']
                
                epoch_losses.append(avg_loss)
                
                # ì—í¬í¬ë³„ ìš”ì•½
                self.logger.info(f"  Epoch time: {epoch_time:.2f}s ({len(self.train_loader) / epoch_time:.1f} batches/s)")
                self.logger.info(f"  Current LR: {current_lr:.2e}")
                
                # ì†ì‹¤ ê°œì„  ì¶”ì´
                if epoch > 0:
                    loss_change = avg_loss - epoch_losses[epoch-1]
                    loss_pct = (loss_change / epoch_losses[epoch-1]) * 100 if epoch_losses[epoch-1] != 0 else 0
                    arrow = "â†“" if loss_change < 0 else "â†‘"
                    self.logger.info(f"  Loss change: {arrow} {abs(loss_change):.4f} ({abs(loss_pct):.2f}%)")
                
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    self.save_checkpoint(epoch, is_best=True)
                    self.logger.info(f"  âœ“ NEW BEST LOSS! Checkpoint saved.\n")
                else:
                    self.save_checkpoint(epoch, is_best=False)
                    self.logger.info(f"  Checkpoint saved.\n")
            
            # 5. ì •ë¦¬ (ë§ˆìŠ¤í‚¹ í†µê³„ëŠ” restore ì „ì— ìˆ˜ì§‘)
            total_training_time = (datetime.now() - training_start).total_seconds()
            self.logger.info("\n[Step 7] Finalizing...")
            
            # âœ… ë§ˆìŠ¤í‚¹ í†µê³„ë¥¼ restore ì „ì— ìˆ˜ì§‘
            total_frozen = 0
            total_trainable = 0
            for layer_idx in sorted(self.masks.keys()):
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj
                if hasattr(target_module, '_warp_mask'):
                    mask = target_module._warp_mask
                    num_frozen = (mask == 1).sum().item()
                    num_trainable = (mask == 0).sum().item()
                    total_frozen += num_frozen
                    total_trainable += num_trainable
            
            # âœ… CRITICAL: restore ì „ì— ìµœì¢… ê°€ì¤‘ì¹˜ ì¬êµ¬ì„± ë° ì €ì¥
            self.logger.info("\n[Step 7.5] Reconstructing final weights from basis_coeff...")
            self._reconstruct_and_save_final_model()
            
            self.restore_original_forwards()
            
            self.logger.info("\n" + "="*70)
            self.logger.info("PHASE 3 TRAINING FINAL SUMMARY - SFT ìŠ¤íƒ€ì¼ GSM8K íŒŒì¸íŠœë‹ ê²°ê³¼")
            self.logger.info("="*70)
            
            # ì „ì²´ ì—í¬í¬ì— ëŒ€í•œ ì†ì‹¤ ë¶„ì„
            loss_array = np.array(epoch_losses)
            initial_loss = loss_array[0]
            final_loss = loss_array[-1]
            best_loss_val = np.min(loss_array)
            total_loss_improvement = initial_loss - final_loss
            total_loss_improvement_pct = (total_loss_improvement / initial_loss) * 100 if initial_loss > 0 else 0
            
            # ì†ì‹¤ ì•ˆì •ì„± ë¶„ì„
            loss_std_epochs = np.std(loss_array)
            loss_monotonic = np.sum(np.diff(loss_array) <= 0)  # ì†ì‹¤ì´ ê°ì†Œí•˜ê±°ë‚˜ ìœ ì§€ëœ ì—í¬í¬ ìˆ˜
            
            self.logger.info(f"  [ì „ì²´ ì†ì‹¤ ë™í–¥]")
            self.logger.info(f"    â€¢ ì´ˆê¸° ì†ì‹¤: {initial_loss:.4f}")
            self.logger.info(f"    â€¢ ìµœì¢… ì†ì‹¤: {final_loss:.4f}")
            self.logger.info(f"    â€¢ ìµœì € ì†ì‹¤: {best_loss_val:.4f}")
            self.logger.info(f"    â€¢ ì´ ê°œì„ ëŸ‰: {total_loss_improvement:.4f} ({total_loss_improvement_pct:.2f}%) {'âœ…' if total_loss_improvement_pct > 3 else 'âš ï¸'}")
            self.logger.info(f"    â€¢ ì—í¬í¬ê°„ ì†ì‹¤ í‘œì¤€í¸ì°¨: {loss_std_epochs:.6f}")
            self.logger.info(f"    â€¢ ì†ì‹¤ ê°ì†Œ ì—í¬í¬: {loss_monotonic}/{len(loss_array)-1} ({'âœ“' if loss_monotonic > len(loss_array)*0.7 else 'âš ï¸'})")
            
            # ìˆ˜ë ´ ì¶”ì´
            if len(loss_array) > 2:
                first_third = np.mean(loss_array[:len(loss_array)//3])
                last_third = np.mean(loss_array[2*len(loss_array)//3:])
                convergence_improvement = first_third - last_third
                convergence_improvement_pct = (convergence_improvement / first_third) * 100 if first_third > 0 else 0
                self.logger.info(f"    â€¢ ì „ë°˜ë¶€ vs í›„ë°˜ë¶€ ìˆ˜ë ´: {convergence_improvement:.4f} ({convergence_improvement_pct:.2f}%)")
            
            self.logger.info(f"\n  [SFT ìŠ¤íƒ€ì¼ í›ˆë ¨ êµ¬ì„±]")
            self.logger.info(f"    â€¢ Per-device batch size: {self.args.batch_size}")
            self.logger.info(f"    â€¢ Gradient accumulation steps: {self.gradient_accumulation_steps}")
            self.logger.info(f"    â€¢ Effective batch size: {self.args.batch_size * self.gradient_accumulation_steps}")
            self.logger.info(f"    â€¢ Gradient checkpointing: Enabled (ë©”ëª¨ë¦¬ ì ˆê°)")
            self.logger.info(f"    â€¢ LR scheduler: Cosine annealing with 5% warmup")
            self.logger.info(f"    â€¢ Max grad norm: 0.3")
            self.logger.info(f"    â€¢ Optimizer: AdamW with weight decay {self.args.weight_decay}")
            
            self.logger.info(f"\n  [í›ˆë ¨ í†µê³„]")
            self.logger.info(f"    â€¢ ì´ í›ˆë ¨ ì‹œê°„: {total_training_time:.2f}s ({total_training_time/60:.2f} min)")
            self.logger.info(f"    â€¢ ì´ ì—í¬í¬: {self.args.epochs}")
            self.logger.info(f"    â€¢ ì—í¬í¬ë‹¹ í‰ê·  ì‹œê°„: {total_training_time/max(self.args.epochs, 1):.2f}s")
            self.logger.info(f"    â€¢ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {self.args.checkpoint_dir}")
            
            self.logger.info(f"\n  [ë³´í˜¸ëœ ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ (WaRP)]")
            self.logger.info(f"    â€¢ ì´ Frozen ì°¨ì›: {total_frozen:,}")
            self.logger.info(f"    â€¢ ì´ Trainable ì°¨ì›: {total_trainable:,}")
            if total_frozen + total_trainable > 0:
                self.logger.info(f"    â€¢ Frozen ë¹„ìœ¨: {total_frozen/(total_frozen+total_trainable)*100:.2f}%")
            else:
                self.logger.info(f"    â€¢ Frozen ë¹„ìœ¨: N/A (ë§ˆìŠ¤í¬ ë°ì´í„° ì—†ìŒ)")
            
            self.logger.info(f"\n  [ìµœì¢… íŒì •: SFT ìŠ¤íƒ€ì¼ GSM8K íŒŒì¸íŠœë‹ ì„±ê³µë„]")
            
            # íŒì • ê¸°ì¤€ ê³„ì‚°
            loss_improvement_ok = total_loss_improvement_pct > 3  # 3% ì´ìƒ ê°œì„ 
            loss_stability_ok = loss_std_epochs < initial_loss * 0.3  # ì—í¬í¬ê°„ ë³€ë™ < 30%
            monotonic_ok = loss_monotonic >= len(loss_array) * 0.5  # 50% ì´ìƒì˜ ì—í¬í¬ì—ì„œ ê°ì†Œ
            
            if loss_improvement_ok and (loss_stability_ok or monotonic_ok):
                self.logger.info(f"    âœ… ì„±ê³µì ì¸ íŒŒì¸íŠœë‹!")
                self.logger.info(f"       - ì†ì‹¤ì´ {total_loss_improvement_pct:.2f}% ê°œì„ ë¨ (ì´ˆê¸°: {initial_loss:.4f} â†’ ìµœì¢…: {final_loss:.4f})")
                self.logger.info(f"       - ì†ì‹¤ì´ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì¶”ì´")
                if total_frozen > 0:
                    self.logger.info(f"       - ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜({total_frozen:,} dims)ëŠ” ë³´í˜¸ë¨")
            elif total_loss_improvement_pct > 1:
                self.logger.info(f"    âš ï¸ íŒŒì¸íŠœë‹ ì§„í–‰ ì¤‘ (ê°œì„ ì´ ì œí•œì )")
                self.logger.info(f"       - ì†ì‹¤ ê°œì„ : {total_loss_improvement_pct:.2f}%")
                self.logger.info(f"       - ì†ì‹¤ ë²”ìœ„: {initial_loss:.4f} â†’ {final_loss:.4f}")
                self.logger.info(f"       - ë” ë§ì€ ì—í¬í¬ ë˜ëŠ” ë°ì´í„° í•„ìš”í•  ìˆ˜ ìˆìŒ")
            else:
                self.logger.info(f"    âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨ ê°€ëŠ¥ì„±")
                self.logger.info(f"       - ì†ì‹¤ ê°œì„  ë¯¸í¡: {total_loss_improvement_pct:.2f}%")
                self.logger.info(f"       - í•™ìŠµë¥  ë˜ëŠ” ë§ˆìŠ¤í‚¹ ì„¤ì • ê²€í†  í•„ìš”")
            
            self.logger.info("="*70 + "\n")
            
        except Exception as e:
            self.logger.error(f"\nâœ— Error in Phase 3: {str(e)}", exc_info=True)
            self.logger.error("Attempting to restore original forwards...")
            try:
                self.restore_original_forwards()
            except Exception as restore_err:
                self.logger.error(f"Failed to restore forwards: {str(restore_err)}")
            raise
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        Args:
            epoch: ì—í¬í¬ ë²ˆí˜¸
            is_best: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì¸ì§€ ì—¬ë¶€
        """
        checkpoint_dir = os.path.join(self.args.checkpoint_dir, 'checkpoints')
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # --- Reconstruct weights from basis_coeff temporarily so saved checkpoints
        # contain standard weight tensors that can be loaded by `from_pretrained`
        # (avoids requiring a custom loader at evaluation time).
        orig_weights = {}
        try:
            for layer_idx in sorted(self.masks.keys()):
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj

                if not hasattr(target_module, 'basis_coeff') or not hasattr(target_module, 'U_matrix'):
                    continue

                # preserve original weight
                orig_weights[layer_idx] = target_module.weight.data.clone()

                # reconstruct final weight = basis_coeff @ U.T
                basis = target_module.basis_coeff.data
                U = target_module.U_matrix
                final_weight = basis @ U.T

                # copy into module.weight (in-place)
                target_module.weight.data.copy_(final_weight)

            # Build checkpoint with reconstructed weights
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'config': vars(self.args),
            }

            # Save usual checkpoint
            save_path = os.path.join(checkpoint_dir, f'phase3_epoch_{epoch:03d}.pt')
            torch.save(checkpoint, save_path)
            self.logger.debug(f"âœ“ Saved checkpoint (reconstructed weights): {save_path}")

            # Save best
            if is_best:
                best_path = os.path.join(checkpoint_dir, 'phase3_best.pt')
                torch.save(checkpoint, best_path)
                self.logger.debug(f"âœ“ Saved best model (reconstructed weights): {best_path}")

        finally:
            # restore original weights to continue training unaffected
            for layer_idx, orig_w in orig_weights.items():
                layer = self.model.model.layers[layer_idx]
                target_module = layer.mlp.down_proj
                target_module.weight.data.copy_(orig_w)
