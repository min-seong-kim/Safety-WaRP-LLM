# Phase 2 ì—…ë°ì´íŠ¸: Fine-tuning + Importance Scoring

## ğŸ”„ ë³€ê²½ ê°œìš”

Phase 2ê°€ **ë¶„ì„ë§Œ í•˜ëŠ” ê²ƒ**ì—ì„œ **fine-tuning + ì¤‘ìš”ë„ ê³„ì‚°ì„ ë™ì‹œì—**í•˜ë„ë¡ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.

### âŒ ì´ì „ Phase 2
```
ì›ë³¸ ëª¨ë¸ (ë³€í™” ì—†ìŒ)
    â†“
ì•ˆì „ ë°ì´í„°ë¡œ gradient ê³„ì‚° (ì—­ì „íŒŒ ì•ˆí•¨)
    â†“
ì¤‘ìš”ë„ ì ìˆ˜ë§Œ ê³„ì‚°
    ê²°ê³¼: ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ì—…ë°ì´íŠ¸ ì•ˆ ë¨ âŒ
```

### âœ… ìƒˆë¡œìš´ Phase 2
```
ì›ë³¸ ëª¨ë¸
    â†“
basis_coeffë¡œ ì¬ë§¤ê°œë³€ìˆ˜í™” (í•™ìŠµ ê°€ëŠ¥)
    â†“
ì•ˆì „ ë°ì´í„°ë¡œ fine-tuning (optimizer.step() ì‹¤í–‰)
    â†“
ë™ì‹œì— ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (|gradient|)
    ê²°ê³¼: ëª¨ë¸ì´ ì•ˆì „ ë°ì´í„°ë¡œ í•™ìŠµë¨ âœ“
```

---

## ğŸ“Š í•µì‹¬ ê°œë…: Weight Space vs Basis Space

### Weight Space (ì›ë³¸)
- `W_original`: ëª¨ë¸ì˜ ì›ë˜ ê°€ì¤‘ì¹˜ (ê³ ì •, ì°¸ê³ ìš©)
- í¬ê¸°: (d_out, d_in) = (4096, 14336)

### Basis Space (ìƒˆë¡œìš´)
- `basis_coeff`: í•™ìŠµ ê°€ëŠ¥í•œ ê³„ìˆ˜ (í•™ìŠµë¨)
  - í¬ê¸°: (d_out, rank) = (4096, 14336)
  - `requires_grad=True` â† **í•™ìŠµ ê°€ëŠ¥**
  
- `U_matrix`: ê³ ì •ëœ basis í–‰ë ¬ (Phase 1ì—ì„œ ê³„ì‚°ë¨)
  - í¬ê¸°: (d_in, rank) = (14336, 14336)
  - `requires_grad=False` â† **ê³ ì •**

### ê´€ê³„ì‹
```
W_reconstructed = basis_coeff @ U^T

Gradient íë¦„:
loss.backward() 
  â†’ âˆ‚L/âˆ‚basis_coeff (ê³„ì‚°ë¨, ì‚¬ìš©ë¨)
  â†’ âˆ‚L/âˆ‚W_original (ê³„ì‚° ì•ˆí•¨, ë¶ˆí•„ìš”)
  â†’ âˆ‚L/âˆ‚U_matrix (ê³„ì‚° ì•ˆí•¨, UëŠ” ê³ ì •)
```

---

## ğŸ”§ ì½”ë“œ êµ¬ì¡°

### 1ï¸âƒ£ `reparameterize_weights()`

**ëª©í‘œ**: weightë¥¼ basis ê³µê°„ìœ¼ë¡œ ë³€í™˜

```python
# Step 1: ì›ë³¸ weight ì €ì¥ (ê³ ì •)
W_original = target_module.weight.data.clone()  # (4096, 14336)
self.original_weights[layer_idx] = W_original

# Step 2: Basis ì¶”ì¶œ (ê³ ì •)
U = self.basis_data[layer_idx]['U']  # (14336, 14336)
target_module.U_matrix = U  # requires_grad=False

# Step 3: basis_coeff ì´ˆê¸°í™” (í•™ìŠµ ê°€ëŠ¥)
basis_coeff_init = W_original @ U  # (4096, 14336)
target_module.basis_coeff = nn.Parameter(basis_coeff_init)  # requires_grad=True

# Forwardì—ì„œ ì‚¬ìš©:
# W = basis_coeff @ U^T
```

### 2ï¸âƒ£ `compute_importance()`

**ëª©í‘œ**: Fine-tuningê³¼ ë™ì‹œì— ì¤‘ìš”ë„ ê³„ì‚°

#### Phase 1: í•™ìŠµ ì¤€ë¹„
```python
# Optimizer ì„¤ì • (basis_coeff íŒŒë¼ë¯¸í„°ë§Œ)
optimizer = AdamW([basis_coeff], lr=1e-5)

# Forward hook: weight ë™ì  ë³µì›
def hook(module, input, output):
    W = module.basis_coeff @ module.U_matrix.T
    module.weight.data = W
    return output
```

#### Phase 2: í›ˆë ¨ ë£¨í”„
```python
for epoch in range(epochs):
    for batch in dataloader:
        # Forward: weight = basis_coeff @ U^T (hookì—ì„œ ìë™)
        outputs = model(input_ids, attention_mask)
        
        # Loss ê³„ì‚°
        loss = CrossEntropyLoss(logits, targets)
        
        # Backward: basis_coeff.grad ê³„ì‚°
        optimizer.zero_grad()
        loss.backward()
        
        # Importance ìˆ˜ì§‘ (backward í›„)
        grad_abs = |basis_coeff.grad|  # Element-wise absolute value
        importances.append(grad_abs)
        
        # Update: basis_coeff ì—…ë°ì´íŠ¸
        optimizer.step()
```

#### Phase 3: Importance í‰ê· 
```python
# ëª¨ë“  ë°°ì¹˜ì˜ gradient ì ˆëŒ“ê°’ ìˆ˜ì§‘ (num_batches, d_out, rank)
layer_importances = stack(importances)

# ë°°ì¹˜ ì¶• í‰ê· 
importance_mean = layer_importances.mean(dim=0)  # (d_out, rank)

# Input ì°¨ì›ë³„ sum (ê° inputì˜ ëˆ„ì  ì˜í–¥)
importance_per_input = importance_mean.sum(dim=0)  # (rank,)
```

---

## ğŸ“ˆ ë¡œê·¸ ì¶œë ¥ í•´ì„

### ì¬ë§¤ê°œë³€ìˆ˜í™” ë‹¨ê³„
```
Layer 31:
  âœ“ W_original (ê³ ì •):     torch.Size([4096, 14336])
  âœ“ basis_coeff (í•™ìŠµ):    torch.Size([4096, 14336])
  âœ“ U_matrix (ê³ ì •):       torch.Size([14336, 14336])
  âœ“ Forward: W = basis_coeff @ U^T
```

### í›ˆë ¨ ë‹¨ê³„
```
Training Setup
  âœ“ Model set to training mode
  âœ“ Optimizer created: AdamW
  - Learning rate: 1e-05
  - Weight decay: 0.01
  - Parameters: 1 basis_coeff tensors
  - Layers: [31]
  âœ“ 1 forward hooks registered

Fine-tuning with Importance Tracking
[Epoch 1/2] Loss: 0.8234  â† basis_coeff ì—…ë°ì´íŠ¸ ì¤‘
[Epoch 2/2] Loss: 0.7891  â† ì†ì‹¤ ê°ì†Œ (fine-tuning ì‘ë™)

Computing Importance Scores
âœ“ Layer 31:
  - Gradient shape (per batch): (d_out, rank) = torch.Size([4096, 14336])
  - Importance aggregated to input-wise (sum): (14336,)
  - Mean: 0.012345
  - Std: 0.005678
```

---

## ğŸ¯ Phase 2 ì‹¤í–‰

### ê¸°ë³¸ ëª…ë ¹ì–´
```bash
python train.py \
    --phase 2 \
    --model_name meta-llama/Llama-3.1-8B-Instruct \
    --basis_dir ./checkpoints/phase1_*/basis \
    --safety_samples 5000 \
    --batch_size 4 \
    --safety_epochs 2 \
    --safety_lr 1e-5 \
    --keep_ratio 0.1 \
    --device cuda \
    --seed 42
```

### íŒŒë¼ë¯¸í„°
| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `--safety_epochs` | 1 | Fine-tuning ì—í¬í¬ ìˆ˜ |
| `--safety_lr` | 1e-5 | basis_coeff í•™ìŠµë¥  |
| `--safety_weight_decay` | 0.01 | Weight decay |
| `--keep_ratio` | 0.1 | ìœ ì§€í•  ì¤‘ìš”ë„ ë¹„ìœ¨ (ìƒìœ„ 10%) |

---

## âœ… ê²€ì¦ ì‚¬í•­

### 1ï¸âƒ£ basis_coeff í•™ìŠµ í™•ì¸
```python
# Phase 2 ì „í›„ basis_coeff ë³€í™” í™•ì¸
before = basis_coeff_init.norm()
# ... í›ˆë ¨ ...
after = basis_coeff.detach().norm()

print(f"basis_coeff norm changed: {before:.4f} â†’ {after:.4f}")
# ê°’ì´ ë³€í–ˆìœ¼ë©´ í•™ìŠµ ì§„í–‰ ì¤‘
```

### 2ï¸âƒ£ Importance ì ìˆ˜ í™•ì¸
```python
# importanceê°€ ì–‘ìˆ˜ ê°’ì¸ì§€ í™•ì¸
importance_min = importance.min()
importance_max = importance.max()

print(f"Importance range: [{importance_min:.6f}, {importance_max:.6f}]")
# ëª¨ë‘ >= 0ì´ì–´ì•¼ í•¨ (ì ˆëŒ“ê°’ì´ë¯€ë¡œ)
```

### 3ï¸âƒ£ ë§ˆìŠ¤í¬ ìƒì„± í™•ì¸
```python
# keep_ratio=0.1ì¼ ë•Œ, ìƒìœ„ 10%ê°€ ë§ˆìŠ¤í¬ë˜ëŠ”ì§€ í™•ì¸
mask_sum = mask.sum().item()
total = len(mask)
actual_ratio = mask_sum / total

print(f"Masked elements: {mask_sum}/{total} ({actual_ratio*100:.1f}%)")
# ~10% ê·¼ì²˜ì—¬ì•¼ í•¨
```

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### âš ï¸ Weight Space í˜¼ë™ ë°©ì§€

```python
# âŒ ì˜ëª»ëœ ì‚¬ìš©
W = target_module.weight  # ì›ë³¸ weight (ì¬êµ¬ì„±ë˜ì§€ ì•Šì€)
# â†’ Hookì´ ìë™ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ í—·ê°ˆë¦¼

# âœ… ì˜¬ë°”ë¥¸ ì‚¬ìš©
# Forward pass ì¤‘ì—ë§Œ ì‚¬ìš© (hookì—ì„œ ìë™ ì²˜ë¦¬)
output = model(input_ids)  # W = basis_coeff @ U^T (ìë™)
```

### âš ï¸ Gradient íë¦„ í™•ì¸

```python
# basis_coeffë§Œ í•™ìŠµë˜ì–´ì•¼ í•¨
optimizer = AdamW([basis_coeff], lr=1e-5)  # âœ“ ì˜¬ë°”ë¦„

# âŒ ì˜ëª»ëœ ë°©ì‹
optimizer = AdamW(model.parameters())  # ì „ì²´ íŒŒë¼ë¯¸í„° í¬í•¨
# â†’ U_matrixì™€ W_originalë„ ì—…ë°ì´íŠ¸ ì‹œë„ (ë¶ˆí•„ìš”)
```

### âš ï¸ Hook ë“±ë¡ ì‹œì 

```python
# Hookì€ forward ì „ì— ë“±ë¡ë˜ì–´ì•¼ í•¨
register_forward_hook(make_forward_hook)

# ê·¸ í›„ model(input_ids)ë¥¼ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ weight ì¬êµ¬ì„±
```

---

## ğŸ“ ì¶œë ¥ íŒŒì¼

Phase 2 ì™„ë£Œ í›„ ë‹¤ìŒ íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤:

```
./checkpoints/phase2_*/
  â””â”€ checkpoints/
      â””â”€ masks/
          â”œâ”€ layer_31_mask.pt      # Binary mask (1: frozen, 0: trainable)
          â””â”€ metadata.json         # ë©”íƒ€ë°ì´í„°
```

---

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„ (Phase 3)

Phase 2ì—ì„œ ìƒì„±ëœ maskë¥¼ Phase 3ì—ì„œ ì‚¬ìš©:

```bash
python train.py \
    --phase 3 \
    --model_name meta-llama/Llama-3.1-8B-Instruct \
    --basis_dir ./checkpoints/phase1_*/basis \
    --masks_dir ./checkpoints/phase2_*/checkpoints/masks \
    --utility_samples 1000 \
    --epochs 3 \
    --device cuda
```

---

## ğŸ’¡ ì¶”ê°€ íŒ

### Phase 2 ì†ì‹¤ì´ ë–¨ì–´ì§€ì§€ ì•Šìœ¼ë©´?

```bash
# í•™ìŠµë¥  ì¦ê°€
--safety_lr 5e-5

# ë” ë§ì€ ì—í¬í¬
--safety_epochs 3

# ë” ë§ì€ ìƒ˜í”Œ
--safety_samples 10000
```

### Importance ì ìˆ˜ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´?

```python
# ì •ê·œí™” ì¶”ê°€ (optional)
importance_normalized = (importance - importance.mean()) / (importance.std() + 1e-8)
```

---

## ğŸ“š ì°¸ê³ 

ì´ êµ¬í˜„ì€ **WaRP-CIFSL ì›ë³¸ ë°©ì‹**ì„ ë”°ë¦…ë‹ˆë‹¤:
- Element-wise gradient ì ˆëŒ“ê°’ ê³„ì‚°
- Per-input ì¤‘ìš”ë„ aggregation
- Quantile ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„±

